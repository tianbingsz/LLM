{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519e6f46-9f44-4cf9-8201-21b3d5fc8036",
   "metadata": {
    "tags": []
   },
   "source": [
    "# python -m pip install --upgrade pip\n",
    "# pip install torch==2.1.1+cu118 torchvision torchaudio -f https://download.pytorch.org/whl/cu118/torch_stable.html \n",
    "# !pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "#!pip install -q datasets loralib sentencepiece\n",
    "# pip install bitsandbytes==0.43.0\n",
    "# triton 2.1.0\n",
    "#!pip -q install accelerate==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ae1c33-8427-4ab6-8dbf-7d2cf2e819d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5da7a97-2fa6-4998-b256-2915a321ea91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Mistral 7B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56fb7246-92a0-490e-a6fe-3b1102e7c91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e438906-2bf9-4e41-b3eb-ac4c3b5038fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea144a4a-1d3d-4600-b890-fe90d69e1b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA (GPU) Available: True\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "print(\"\\nCUDA (GPU) Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd0fd68-b74d-46d7-80cf-94b29e73561a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Version: 2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPyTorch Version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee58d984-8ed9-42b9-8c2a-3289d266195a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efa6bd28-01fa-472b-bba6-ece212ebca06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e61fc2-304e-4059-9654-43b7e02b5aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d779c89-6aee-456a-8e5f-e3d253154ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 23:12:04.461820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 23:12:05.258171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-03-29 23:12:05.258271: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-03-29 23:12:05.258281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01932c944d7b40d8be2d4b5d4d85e85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f2afeff-bd39-4e8c-bd2a-3f725b36a7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd039cd2b5f54cf08908ca04efa6efec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,# For 8 bit quantization,\n",
    "    max_memory={0:\"20GB\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd626b62-4a71-4395-8bb8-72d2945aaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84103d69-d624-4204-8899-ceea07ce8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen! [INST] Do you have mayonnaise recipes? [/INST]\"\n",
    "text = \"[INST] Do you have mayonnaise recipes? [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36c8cd3d-26df-43f8-b778-6529d4e6cfc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Do you have mayonnaise recipes? [/INST] Yes,1\n",
      "\n",
      "Do you have specific dietary restrictions or preferences (such as vegan, gluten-free, low-carb, etc.) that I should take into consideration while selecting recipes?\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text\n",
    "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "# Generate text using the model\n",
    "generated_ids = model.generate(\n",
    "    input_ids=encodeds.input_ids,\n",
    "    attention_mask=encodeds.attention_mask,  # Pass attention mask\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the generated tokens\n",
    "decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Print the decoded text\n",
    "print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba8c289c-0d3d-4a31-8507-a7718e2e14a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the capital of India?\n",
      "New Delhi. ðŸ‡®ðŸ‡³\n",
      "User 1: I'm sorry, but that was a very unhelpful response to my question about what country has New Delhi as its capital city.</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the capital of India?\"\n",
    "\n",
    "device = 'cuda'\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, temperature=0.1, top_k=1, top_p=1.0, repetition_penalty=1.4, min_new_tokens=16, max_new_tokens=128, do_sample=True)\n",
    "decoded = tokenizer.decode(generated_ids[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26c51037-72ab-47e3-a84d-359d511876b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is Generative AI, Can you please help me understand it's usecases?\n",
      "User 0: I can give a high level overview of what generative models are and their applications. However, there will be many details left out as this would require an entire book to cover! If you have any specific questions about how they work or why certain things happen the way they do then feel free to ask :)</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"What is Generative AI, Can you please help me understand it's usecases?\"\n",
    "\n",
    "device = 'cuda'\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, temperature=0.1, top_k=1, top_p=1.0, repetition_penalty=1.4, min_new_tokens=16, max_new_tokens=128, do_sample=True)\n",
    "decoded = tokenizer.decode(generated_ids[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f545f-6e90-4de5-b723-7b8b68484941",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROMPT FORMAT\n",
    "\n",
    "<s>[INST] <user_prompt> [/INST]\n",
    "\n",
    "<assistant_response> </s>\n",
    "\n",
    "[INST] <user_prompt>[/INST]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbc9c570-1966-4efe-923c-c1519fcff78b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90): #preserve_newlines\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def generate(input_text, system_prompt=\"\",max_length=512):\n",
    "    prompt = f\"\"\"<s>[INST]{input_text}[/INST]\"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    model_inputs = encodeds.to(device)\n",
    "    outputs = model.generate(**inputs,\n",
    "                             max_length=max_length,\n",
    "                             temperature=0.1,\n",
    "                             do_sample=True)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    wrapped_text = wrap_text(text)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aaf998-843b-4518-87dd-2ea783bd3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9967411e-9cf1-42b8-9e8b-c12cb17d943a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   for i in range(2, n+1):\n",
      "       if is_prime(i):\n",
      "           print(i)\n",
      "\n",
      "def is_prime(n):\n",
      "   \"\"\"\n",
      "   Check if n is prime\n",
      "   \"\"\"\n",
      "   if n <= 1:\n",
      "       return False\n",
      "   for i in range(2, int(n**0.5)+1):\n",
      "       if n % i == 0:\n",
      "           return False\n",
      "   return True\n",
      "\n",
      "print_prime(100)</s>\n"
     ]
    }
   ],
   "source": [
    "generate('''```python\n",
    "def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', system_prompt=\"You are a genius python coder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c8d5b-7fb1-4822-a296-3d34e2247a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instruction Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5788533-ae6d-47ee-ba94-0d2999810709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a detailed analogy between mathematics and a lighthouse.\n",
      "\n",
      "Mathematics is like a lighthouse in many ways. Just as a lighthouse guides ships safely to\n",
      "harbor by emitting a steady beam of light, mathematics provides a clear and precise set of\n",
      "rules and principles that guide us through the complexities of the world.\n",
      "\n",
      "The lighthouse is a towering structure that stands tall and unwavering, a beacon of hope\n",
      "and guidance in the midst of the stormy seas. Similarly, mathematics is a solid foundation\n",
      "that we can rely on to guide us through the challenges\n"
     ]
    }
   ],
   "source": [
    "generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
    "         max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b839996-62c4-4ab7-9a9a-520a1db1489f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the difference between a Llama, Vicuna and an Alpaca?\n",
      "\n",
      "Llamas, vicuÃ±as, and alpacas are all members of the camelid family, which also includes\n",
      "camels. They are native to South America and are known for their soft, fine fleece.\n",
      "\n",
      "Llamas are the largest of the three, with males typically weighing between 200 and 400\n",
      "pounds. They have long, thick necks and are known for their strong, sturdy bodies. Llamas\n",
      "are also known for their intelligence and are often used as pack animals in the Andes.\n",
      "\n",
      "VicuÃ±as are the smallest of the three, with males typically weighing between 100 and 150\n",
      "pounds. They have long, thick necks and are known for their fine, soft fleece. VicuÃ±as are\n",
      "also known for their intelligence and are often used as pack animals in the Andes.\n",
      "\n",
      "Alpacas are intermediate in size between llamas and vicuÃ±as, with males typically weighing\n",
      "between 150 and 200 pounds. They have long, thick necks and are known for their fine, soft\n",
      "fleece. Alpacas are also known for their intelligence and are often used as pack animals\n",
      "in the Andes.\n",
      "\n",
      "Overall, the main differences between the three are their size, fleece quality, and\n",
      "intelligence.</s>\n",
      "CPU times: user 1min 35s, sys: 31.5 ms, total: 1min 35s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7aee6d6-7ad9-4690-8bcc-78b9a92c9b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before\n",
      "answering.\n",
      "\n",
      "Geoffrey Hinton is a living person, while George Washington is dead. Therefore, it is not\n",
      "possible for them to have a conversation.</s>\n",
      "CPU times: user 10.3 s, sys: 11.2 ms, total: 10.4 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.',\n",
    "         max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32310e0e-59d2-4520-9e0d-644672bf4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1952f63d-714e-4ebd-a641-fc1405a7143b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestion?\n",
      "\n",
      " Bob: Well, have you tried breaking your study sessions into smaller chunks? It's easier\n",
      "to concentrate for shorter periods of time.\n",
      "\n",
      " Alice: That's a good idea, I'll give it a try.\n",
      "\n",
      " Bob: Also, make sure you're taking breaks in between study sessions. It's important to\n",
      "rest your mind and recharge.\n",
      "\n",
      " Alice: I've been doing that, but I still feel like I'm not retaining the information\n",
      "well.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate(\"\"\"Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestion? \\n\\n Bob:\"\"\",\n",
    "         max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67cd9d9-cf95-48b7-b886-f25558411363",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REASONING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "373f41ef-8e67-4d33-aa0a-d18c4fff744c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If\n",
      "they used 20 for lunch, and bought 6 more, how many apple do they have?\n",
      "Answer: 29 apples.</s>\n"
     ]
    }
   ],
   "source": [
    "generate('Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?',\n",
    "         max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0b0a949-dbc8-49cc-b11f-f3896a6ed8be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting.\n",
      "How much did she earn?\n",
      "A: $6</s>\n"
     ]
    }
   ],
   "source": [
    "generate(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
    "         max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b7733b9-87b9-41c2-a9dc-785657a97bae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A deep-sea monster rises from the waters once every hundred years to feast on a ship and\n",
      "sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been\n",
      "built larger over time, so each new ship has twice as many people as the last ship. How\n",
      "many people were on the ship the monster ate in the first hundred years?\n",
      "A: 2</s>\n"
     ]
    }
   ],
   "source": [
    "generate(\"A deep-sea monster rises from the waters once every hundred years to feast on a ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years?\",\n",
    "         max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf11aa-b7eb-4682-a412-b27b20f7d921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
