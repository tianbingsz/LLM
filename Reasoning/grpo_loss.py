# -*- coding: utf-8 -*-
"""grpo_loss.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vvm4DN7gqU6FIFesl4oBJOex9Bt93Dwv

# Implementation of GRPO Loss
"""

import torch
import torch.nn.functional as F

"""## Compute GRPO or Reinforce Advantage"""

from collections import defaultdict


def compute_grpo_outcome_advantage(token_level_rewards: torch.Tensor,
                                   eos_mask: torch.Tensor,
                                   index: torch.Tensor,
                                   epsilon: float = 1e-6):
    """
    Compute advantage for GRPO, operating only on Outcome reward
    (with only one scalar reward for each response).
    Average/std across samples (batches) within each group

    Args:
        token_level_rewards: `(torch.Tensor)`
            shape: (bs, response_length)
        eos_mask: `(torch.Tensor)`
            shape: (bs, response_length)
    Returns:
        advantages: `(torch.Tensor)`
            shape: (bs, response_length)
        Returns: `(torch.Tensor)`
            shape: (bs, response_length)
    """
    response_length = token_level_rewards.shape[-1]
    scores = token_level_rewards.sum(dim=-1)

    id2score = defaultdict(list)
    id2mean = {}
    id2std = {}

    with torch.no_grad():
        bsz = scores.shape[0]
        for i in range(bsz):
            id2score[index[i]].append(scores[i])
        for idx in id2score:
            if len(id2score[idx]) == 1:
                id2mean[idx] = torch.tensor(0.0)
                id2std[idx] = torch.tensor(1.0)
            elif len(id2score[idx]) > 1:
                id2mean[idx] = torch.mean(torch.tensor(id2score[idx]))
                id2std[idx] = torch.std(torch.tensor([id2score[idx]]))
            else:
                raise ValueError(f"no score in prompt index: {idx}")
        for i in range(bsz):
            scores[i] = (scores[i] - id2mean[index[i]]) / (id2std[index[i]] + epsilon)
        scores = scores.unsqueeze(-1).expand(-1, response_length) * eos_mask

    return scores, scores

def reinforce_advantage(rewards, eps=1e-8, mask=None):
    """
    Normalize rewards (advantages) across the batch dimension (dim=0) per time step.

    Args:
        rewards: Tensor of shape (B, T) containing rewards or advantages.
        mask: (B, T), T = input_len + response_len
        eps: Small constant for numerical stability.
        mask: Optional mask tensor of shape (B, T), with 1 for valid tokens and 0 for padding.

    Returns:
        Normalized advantages tensor of shape (B, T).
    """

    if mask is None:
        xmean = torch.mean(rewards, dim=0, keepdim=True) # (B, T)
        # Use population variance for stability
        xvar = torch.var(rewards, dim=0, keepdim=True, unbiased=False) # (B, T)
        adv = (rewards - xmean) / torch.sqrt(xvar + eps)
        return adv

    # mask
    valid_cnt = torch.sum(mask, dim=0, keepdim=True).clamp(min=1.0)
    masked_rewards = rewards * mask
    xmean = torch.sum(masked_rewards, dim=0, keepdim=True) / valid_cnt
    # Variance: E[(x - mean)^2]
    xvar = torch.sum((masked_rewards - xmean) ** 2 * mask, dim=0, keepdim=True) /valid_cnt
    adv = (masked_rewards - xmean) / torch.sqrt(xvar + eps)
    return adv * mask # zero out padded positions

B, T = 4, 8

input_len, response_len = 6, 2
 # mask, input 0, reponse 1
mask = torch.zeros(B, T)
mask[:, input_len:] = 1

# (B, T) advantage for each sample response sequence (B, response_len)
rewards = torch.randn(B, T)
index = [idx for idx in range(T)]
# grpo
_, advs = compute_grpo_outcome_advantage(token_level_rewards=rewards, index=index, eos_mask=mask)
print(advs)
# reinforce
adv = reinforce_advantage(rewards=rewards, mask=mask)
print(adv)

"""## GRPO Loss"""

# KL(pi || pi_ref) = pi_ref/pi - log(pi_ref/pi) - 1
def compute_KL(pi_logprob, pi_ref_logprob):
    return pi_ref_logprob.exp() / pi_logprob.exp()- (pi_ref_logprob - pi_logprob) - 1

def grpo_loss(pi_logprob, pi_old_logprob, pi_ref_logprob, advantage, mask, response_len, eps = 0.2, beta=0.01):
    """
        L = - 1/G \sum_i 1/o_i \sum_t min(r(i,t) * A(i,t), clip (i,t) * A(i,t)) - \beta KL(\pi || \pi_ref)
        r(i,t) = pi(i,t)/pi_old(i,t)  (B, T)
        clip(i,t) = clip(r(i,t), 1 - eps, 1 + eps)
        pi_logprob, pi_old_logprob, pi_ref_logporb: (B, T)
        advantage (B, T), adv for each group at each token t (repsone token not masked)
        input_len: num tokens of input
        response_len: num of ouput tokens for group, scalar

        Return: loss scalar averaged from (B, T) with mask
    """
    B, T = pi_logprob.shape
    # len (output tokens) for each group
    response_len = torch.tensor([response_len] * B, dtype = torch.long) # (B, )

    # GRPO loss
    ratio = torch.exp(pi_logprob - pi_old_logprob) # (B, T)
    ratio_clip = torch.clamp(ratio, 1 - eps, 1 + eps) # (B, T)
    policy_gradient = torch.min(ratio * advantage, ratio_clip * advantage)#(B, T)
    kl = compute_KL(pi_logprob, pi_ref_logprob)
    # mask for output response tokens
    loss = (policy_gradient -  beta * kl) * mask # (B, T)
    # maxmize the surrogate objective -> min hte negative surrogate loss
    loss = - 1/B * (1/response_len.unsqueeze(dim=-1)) * loss # (B, T)
    loss = loss.sum()

    return loss

"""### example"""

B, T, V = 4, 8, 32
pi_logits = torch.randn(B, T, V)
pi_old_logits = torch.randn(B, T, V)
pi_ref_logits = torch.randn(B, T, V)

pi_logprob = F.log_softmax(pi_logits, dim=-1) # (B, T, V)
pi_old_logprob = F.log_softmax(pi_old_logits, dim=-1)
pi_ref_logprob = F.log_softmax(pi_ref_logits, dim=-1)

# group outputs (responses) for each input question
token_ids = torch.tensor([[8, 9, 10, 11, 12, 13, 14, 15], # group 1, input : 8,9, 10, 11,12,13, output :14, 15
                          [8, 9, 10, 11, 12, 13, 15, 16],
                          [8, 9, 10, 11, 12, 13, 16, 17],
                          [8, 9, 10, 11, 12, 13, 17, 18],
                        ]) # (B, T)

# toke-leval log prob
pi_logprob = torch.gather(pi_logprob, dim=-1, index=token_ids.unsqueeze(-1)).squeeze(-1) # (B, T)
pi_old_logprob = torch.gather(pi_old_logprob, dim=-1, index=token_ids.unsqueeze(-1)).squeeze(-1)
pi_ref_logprob = torch.gather(pi_ref_logprob, dim=-1, index=token_ids.unsqueeze(-1)).squeeze(-1)


input_len, response_len = 6, 2
# token-level rewards
rewards = torch.randn(B, T)
mask = torch.zeros(B, T)
mask[:, input_len:] = 1 # response 1, input 0

advantage = reinforce_advantage(rewards=rewards, mask=mask)
loss = grpo_loss(pi_logprob, pi_old_logprob, pi_ref_logprob, advantage, mask, response_len)
print(loss)