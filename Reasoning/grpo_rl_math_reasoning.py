# -*- coding: utf-8 -*-
"""GRPO_RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jwO88lXYWEgr7ABnaRM8c5ZeQPx5xnUz

CS366: GRPO
https://github.com/stanford-cs336/assignment5-alignment/blob/main/cs336_spring2025_assignment5_alignment.pdf

**1. DATASET and Toekenization**
"""

!pip install latex2sympy2_extended
!pip install math_verify
!pip install pylatexenc

R1_ZERO_PROMPT = """
A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.
The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.
User: {question}
Assistant: <think>
"""

import os
from typing import Any, Callable, Literal

import torch
from torch import Tensor
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import PreTrainedTokenizerBase

def tokenize_prompt_and_output(
    prompt_strs: list[str],
    output_strs: list[str],
    tokenizer: PreTrainedTokenizerBase,
) -> dict[str, Tensor]:
    """Tokenize the prompt and output strings, and construct a mask that is 1
    for the response tokens and 0 for other tokens (prompt or padding).

    Args:
        prompt_strs: list[str], the prompt strings.
        output_strs: list[str], the output strings.
        tokenizer: PreTrainedTokenizer, the tokenizer to use.

    Returns:
        dict[str, torch.Tensor]:
            "input_ids": torch.Tensor of shape (batch_size, max(prompt_and_output_lens) - 1):
                the tokenized prompt and output strings, with the final token sliced off.
            "labels": torch.Tensor of shape (batch_size, max(prompt_and_output_lens) - 1):
                shifted input_ids (i.e., the input_ids without the first token).
            "response_mask": torch.Tensor of shape (batch_size, max(prompt_and_output_lens) - 1):
                a mask on the response tokens in `labels`.
    """
    input_ids_list = []
    response_mask_list = []

    for prompt, output in zip(prompt_strs, output_strs):
        prompt_enc = tokenizer(prompt, add_special_tokens=False)
        output_enc = tokenizer(output, add_special_tokens=False)
        full_input = prompt_enc['input_ids'] + output_enc['input_ids']
        response_mask = [0] * len(prompt_enc['input_ids']) + [1] * len(output_enc['input_ids'])
        input_ids_list.append(torch.tensor(full_input, dtype=torch.long))
        response_mask_list.append(torch.tensor(response_mask, dtype=torch.long))

    batch_size = len(input_ids_list)
    max_len = max(len(ids) for ids in input_ids_list)
    input_ids_batch = torch.full((batch_size, max_len), tokenizer.pad_token_id, dtype=torch.long)
    response_mask_batch = torch.zeros((batch_size, max_len), dtype=torch.long)

    for i, (ids, mask) in enumerate(zip(input_ids_list, response_mask_list)):
        seq_len = len(ids)
        input_ids_batch[i, :seq_len] = ids
        response_mask_batch[i, :seq_len] = mask

    return {
        "input_ids": input_ids_batch[:, :-1],               # (batch, max_len-1)
        "labels": input_ids_batch[:, 1:],                   # (batch, max_len-1)
        "response_mask": response_mask_batch[:, 1:]         # (batch, max_len-1)
    }

import json

class MathRLDataset(Dataset):
    def __init__(self, jsonl_path: str, tokenizer: AutoTokenizer):
        self.jsonl_path = jsonl_path
        self.tokenizer = tokenizer
        questions, answers = self._load_math_data()

        # Store prompts and groundtruths
        self.prompts = [R1_ZERO_PROMPT.format(question=question) for question in questions]
        self.groundtruths = answers

        # Pre-tokenize prompt + groundtruth for optional supervised baseline training
        tokenized = tokenize_prompt_and_output(self.prompts, self.groundtruths, tokenizer)
        self.input_ids = tokenized["input_ids"] # (B, T)
        self.labels = tokenized["labels"] # (B, T)
        self.response_mask = tokenized["response_mask"] # (B, T)

    def __len__(self):
        return len(self.prompts)

    def __getitem__(self, idx):
        return {
            "prompt_str": self.prompts[idx],
            "groundtruth_str": self.groundtruths[idx],
            "input_ids": self.input_ids[idx],
            "labels": self.labels[idx],
            "response_mask": self.response_mask[idx],
            "index": idx,
        }

    def _load_math_data(self):
        questions, answers = [], []
        with open(self.jsonl_path, 'r') as f:
            for line in f.readlines():
                example = json.loads(line)
                questions.append(example['question'])
                answers.append(example['answer'])

        print(f"Loaded {len(questions)} MATH examples.")
        return questions, answers

"""**2. Reward Computes**"""

def compute_group_normalized_rewards(
    reward_fn: Callable,
    rollout_responses: list[str],
    repeated_ground_truths: list[str],
    group_size: int,
    advantage_eps: float,
    normalize_by_std: bool,
) -> tuple[torch.Tensor, dict[str, float]]:
    """
    Compute rewards for each group of rollout responses,
    normalized by the group size.

    For more on GRPO, see:
        DeepSeekMath: https://arxiv.org/abs/2402.03300
        DeepSeek-R1: https://arxiv.org/abs/2501.12948

    Args:
        reward_fn: Callable[[str, str], dict[str, float]],
            scores the rollout responses against the ground truths,
            producing a dict with keys
            "reward", "format_reward", and "answer_reward".
        rollout_responses: list[str], rollouts from the policy.
            The length of this list is
            `rollout_batch_size = n_prompts_per_rollout_batch * group_size`.
        repeated_ground_truths: list[str], the ground truths for the examples.
            The length of this list is `rollout_batch_size`,
            because the ground truth for each example is repeated `group_size` times.
        group_size: int, number of rollouts per group.
        advantage_eps: float, epsilon to avoid division by zero
            during group normalization.
        normalize_by_std: bool, whether to normalize the rewards by
            std(rewards).

    Returns:
        tuple[torch.Tensor, torch.Tensor, dict[str, float]]:
            torch.Tensor of shape (rollout_batch_size,):
                group-normalized rewards for each rollout response.
            torch.Tensor of shape (rollout_batch_size,):
                raw rewards for each rollout response.
            dict[str, float]: metadata for the rewards of the rollout batch.
                You may choose what you wish to log here
                (some statistics of the rewards, etc.).
    """
    # 1. Compute raw rewards for all responses
    raw_rewards = []
    format_rewards = []
    answer_rewards = []
    for response, gt in zip(rollout_responses, repeated_ground_truths):
        reward_dict = reward_fn(response, gt)
        raw_rewards.append(reward_dict["reward"])
        format_rewards.append(reward_dict["format_reward"])
        answer_rewards.append(reward_dict["answer_reward"])
    raw_rewards = torch.tensor(raw_rewards, dtype=torch.float32)

    # 2. Group normalization
    N = len(raw_rewards)
    assert N % group_size == 0, "Rollout batch size must be divisible by group_size"
    n_groups = N // group_size


    # grouping: [n_groups, group_size]
    group_rewards = raw_rewards.view(n_groups, group_size)
    group_means = group_rewards.mean(dim=1, keepdim=True)
    if normalize_by_std:
        group_stds = group_rewards.std(dim=1, keepdim=True)
        denom = group_stds + advantage_eps
    else:
        denom = 1.0

    # normalization
    normalized_groups = (group_rewards - group_means) / denom       # [n_groups, group_size]
    normalized_rewards = normalized_groups.view(N)                  # (N,)

    # 3. Optional: Collect some statistics
    metadata = {
        "reward_mean": float(raw_rewards.mean()),
        "reward_std": float(raw_rewards.std()),
        "reward_max": float(raw_rewards.max()),
        "reward_min": float(raw_rewards.min()),
        "format_reward_mean": float(np.mean(format_rewards)),
        "answer_reward_mean": float(np.mean(answer_rewards)),
    }

    return normalized_rewards, raw_rewards, metadata

"""**3. Log Probs and Entropy**"""

def compute_entropy(logits: torch.Tensor) -> torch.Tensor:
    """Get the entropy of the logits (i.e., entropy of the final dimension).
        This avoids computing logpi, which is numerically unstable when pi is very small
        H(p) = - sum_i pi logpi
        pi = exp(zi) / logsumexp(z)
        logsumexp(z) = log sum_j exp(z_j)
        entropy = logsumexp(logits) - sum_i p_i * logits_i

    """
    # Numerically stable computation of entropy
    lse = torch.logsumexp(logits, dim=-1)
    probs = torch.softmax(logits, dim=-1)
    expected_logit = (probs * logits).sum(dim=-1)
    entropy = lse - expected_logit
    return entropy

def get_response_log_probs(
    model: torch.nn.Module,
    input_ids: torch.Tensor,
    labels: torch.Tensor,
    return_token_entropy: bool,
) -> torch.Tensor:
    """Get the conditional log-probs of the response given the prompt,
        and optionally the entropy of the next token predictions.

    Args:
        model: PreTrainedModel, the model to score.
        input_ids: torch.Tensor of shape (batch_size, sequence_length):
            the tokenized prompt and output.
        labels: torch.Tensor of shape (batch_size, sequence_length):
            shifted input_ids.
        return_token_entropy: bool, whether to return the entropy of the
            next token predictions.

    Returns:
        dict[str, torch.Tensor]:
            "log_probs": torch.Tensor of shape (batch_size, sequence_length):
                the conditional log-probs of the response given the prompt.
                Note that we have not masked out the token indices corresponding
                to the prompt or padding; that is done in the train loop.
            "token_entropy": Optional[torch.Tensor] of shape (batch_size, sequence_length):
                the entropy of the next token predictions. As with the log-probs,
                we have not masked out the token indices corresponding to the prompt
                or padding; that is done in the train loop.
    """
    # Get logits from model
    outputs = model(input_ids)
    logits = outputs.logits  # (batch_size, seq_len, vocab_size)

    # Compute log-probabilities for each token in the labels
    log_probs = F.log_softmax(logits, dim=-1)  # (batch_size, seq_len, vocab_size)

    # Gather log-probabilities at the correct label indices
    # Unsqueeze `labels` to match log_probs' last dim for gather
    # Result shape: (batch_size, seq_len)
    log_probs_for_labels = torch.gather(
        log_probs, dim=2, index=labels.unsqueeze(-1)
    ).squeeze(-1)

    result = {
        "log_probs": log_probs_for_labels
    }

    if return_token_entropy:
        token_entropy = compute_entropy(logits)  # (batch_size, seq_len)
        result["token_entropy"] = token_entropy

    return result

"""**4. Policy Gradient and Clip loss**"""

def compute_naive_policy_gradient_loss(
    raw_rewards_or_advantages: torch.Tensor,
    policy_log_probs: torch.Tensor,
) -> torch.Tensor:
    """Compute policy gradient loss using either raw rewards or advantages.

    Args:
        raw_rewards_or_advantages: torch.Tensor of shape (batch_size, 1):
            the raw rewards or advantages for each rollout response.
        policy_log_probs: torch.Tensor of shape (batch_size, sequence_length):
            the log-probs of the policy.

    Returns:
        torch.Tensor of shape (batch_size, sequence_length):
            the policy gradient per-token loss.
    """
    seq_length = policy_log_probs.shape[1]
    rewards_or_advantages = raw_rewards_or_advantages.expand(-1, seq_length)
    pg_loss = -policy_log_probs * rewards_or_advantages
    return pg_loss

def compute_grpo_clip_loss(
    advantages: torch.Tensor,
    policy_log_probs: torch.Tensor,
    old_log_probs: torch.Tensor,
    cliprange: float,
) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
    """Compute the GRPO-Clip loss.

    Args:
        advantages: torch.Tensor of shape (batch_size, 1):
            the advantages for each rollout response.
        policy_log_probs: torch.Tensor of shape (batch_size, sequence_length):
            the log-probs of the policy.
        old_log_probs: torch.Tensor of shape (batch_size, sequence_length):
            the log-probs of the old policy.
        cliprange: float, the clip range for the ratio.

    Returns:
        tuple[torch.Tensor, dict[str, torch.Tensor]]:
            torch.Tensor of shape (batch_size, sequence_length):
                the GRPO-Clip per-token loss.
            dict[str, torch.Tensor]: metadata for the GRPO-Clip loss
                (used to compute clip fraction).
    """
    seq_length = policy_log_probs.shape[1]
    advantages = advantages.expand(-1, seq_length)

    ratio = torch.exp(policy_log_probs - old_log_probs) # (batch_size, seq_length)
    clipped_ratio = torch.clamp(ratio, 1 - cliprange, 1 + cliprange)
    lhs, rhs = ratio * advantages, clipped_ratio * advantages
    loss = -torch.min(lhs, rhs)

    metadata = {
        "clipped": (rhs < lhs).float(),
    }

    return loss, metadata

def compute_policy_gradient_loss(
    policy_log_probs: torch.Tensor,
    loss_type: str,
    raw_rewards: torch.Tensor,
    advantages: torch.Tensor,
    old_log_probs: torch.Tensor,
    cliprange: float,
) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
    """
    Wrapper that delegates to the appropriate policy gradient loss function above.
    """
    batch_size, seq_length = policy_log_probs.shape
    if loss_type == "no_baseline":
        assert raw_rewards is not None, "raw_rewards required for no_baseline"
        assert raw_rewards.shape == (batch_size, 1), "raw_rewards must have shape (batch_size, 1)"
        loss = compute_naive_policy_gradient_loss(raw_rewards, policy_log_probs)
        meta = {}
    elif loss_type == "reinforce_with_baseline":
        assert advantages is not None, "advantages required for reinforce_with_baseline"
        assert advantages.shape == (batch_size, 1), "advantages must have shape (batch_size, 1)"
        loss = compute_naive_policy_gradient_loss(advantages, policy_log_probs)
        meta = {}
    elif loss_type == "grpo_clip":
        assert advantages is not None, "advantages required for grpo_clip"
        assert old_log_probs is not None, "old_log_probs required for grpo_clip"
        assert cliprange is not None, "cliprange required for grpo_clip"
        assert old_log_probs.shape == (batch_size, seq_length), "old_log_probs must have shape (batch_size, seq_length)"
        loss, meta = compute_grpo_clip_loss(advantages, policy_log_probs, old_log_probs, cliprange)
    else:
        raise ValueError(f"Unknown loss_type: {loss_type}")
    return loss, meta

def masked_mean(tensor: torch.Tensor, mask: torch.Tensor, dim: int | None = None) -> torch.Tensor:
    """Compute the mean of the tensor along a dimension,
    considering only the elements with mask value 1.

    Args:
        tensor: torch.Tensor, the tensor to compute the mean of.
        mask: torch.Tensor, the mask. We only take the mean over
            the elements with mask value 1.
        dim: int | None, the dimension to compute the mean along.
            If None, sum over all non-masked elements and average
            by their total count.

    Returns:
        torch.Tensor, the mean of the tensor along the specified
            dimension, considering only the elements with mask value 1.
    """
    n_tokens = mask.sum(dim=dim)
    masked_tensor = tensor * mask
    return masked_tensor.sum(dim=dim) / n_tokens

"""**5. Micro Training Step**"""

def grpo_microbatch_train_step(
    policy_log_probs: torch.Tensor,
    response_mask: torch.Tensor,
    gradient_accumulation_steps: int,
    loss_type: Literal["no_baseline", "reinforce_with_baseline", "grpo_clip"],
    raw_rewards: torch.Tensor | None = None,
    advantages: torch.Tensor | None = None,
    old_log_probs: torch.Tensor | None = None,
    cliprange: float | None = None,
) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
    """Compute the policy gradient loss and backprop its gradients for a microbatch.

    Args:
        policy_log_probs: torch.Tensor of shape (batch_size, sequence_length):
            the log-probs of the policy.
        response_mask: torch.Tensor of shape (batch_size, sequence_length):
            the mask for the response.
        gradient_accumulation_steps: int, the number of gradient accumulation steps.
        loss_type: Literal["no_baseline", "reinforce_with_baseline", "grpo_clip"],
            the type of loss function to use.
        raw_rewards: torch.Tensor | None, the raw rewards for each rollout response.
            Needed for loss_type="no_baseline".
        advantages: torch.Tensor | None, the advantages for each rollout response.
            Needed for loss_type in {"reinforce_with_baseline", "grpo_clip"}.
        old_log_probs: torch.Tensor | None, the log-probs of the old policy.
            Needed for loss_type="grpo_clip".
        cliprange: float | None, the clip range for the ratio.
            Needed for loss_type="grpo_clip".
        constant_normalize_factor: int | None, provided if we want to sum over
            the sequence dimension and normalize by this constant factor
            (as in Dr. GRPO).

    Returns:
        tuple[torch.Tensor, dict[str, torch.Tensor]]:
            the policy gradient loss and its metadata.
    """

    # 1. per token loss (batch, seq)
    loss_per_token, meta = compute_policy_gradient_loss(
        policy_log_probs=policy_log_probs,
        loss_type=loss_type,
        raw_rewards=raw_rewards,
        advantages=advantages,
        old_log_probs=old_log_probs,
        cliprange=cliprange,
    )

    # 2. response_mask loss (batch, )
    loss_per_example = masked_mean(loss_per_token, response_mask, dim=1)  # (batch, )

    # 3. batch average: scalar
    loss = loss_per_example.mean()

    # 4. adjust for grad accumulation
    loss = loss / gradient_accumulation_steps

    # 5. backward
    loss.backward()

    meta = meta.copy()
    meta["microbatch_loss"] = loss.detach()
    meta["loss_per_example"] = loss_per_example.detach()

    return loss, meta

def masked_normalize(
    tensor: torch.Tensor,
    mask: torch.Tensor,
    dim: int | None = None,
    normalize_constant: float = 1.0,
) -> torch.Tensor:
    """Sum over a dimension and normalize by a constant,
    considering only the elements with mask value 1.

    Args:
        tensor: torch.Tensor, the tensor to sum and normalize.
        mask: torch.Tensor, the mask. We only consider elements
            with mask value 1.
        dim: int | None, the dimension to sum along before
            normalization. If None, sum over all dimensions.
        normalize_constant: float, the constant to divide by
            for normalization.

    Returns:
        torch.Tensor, the normalized sum, where masked elements
            (mask=0) don't contribute to the sum.
    """
    masked_tensor = tensor * mask
    sum_vals = masked_tensor.sum(dim=dim)
    normalized = sum_vals / normalize_constant
    return normalized

"""**6. Trajectory Generation**"""

def sample_g_outputs(
    policy_model: torch.nn.Module,
    tokenizer,
    prompt_strs: list[str],
    G: int,
    device: torch.device,
    max_new_tokens: int = 32,
    temperature: float = 1.0,
    top_k: int = 50,
) -> list[str]:
    """
    Trajectories generation based on the policy model
    Sample G outputs per prompt string from the policy model.

    Args:
        policy_model: The policy model with a `.generate()` method.
        tokenizer: Tokenizer to encode prompts and decode outputs.
        prompt_strs: List of prompt strings to generate from.
        G: Number of samples per prompt.
        device: Device to run model on.
        max_new_tokens: Max tokens to generate for each sample.
        temperature: Sampling temperature.
        top_k: Top-k sampling parameter.

    Returns:
        List of generated strings of length len(prompt_strs) * G.
    """
    all_sampled_outputs = []
    for _ in range(G):
        prompt_inputs = tokenizer(prompt_strs, return_tensors="pt", padding=True, truncation=True).to(device)
        sampled = policy_model.generate(
            **prompt_inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_k=top_k,
        )
        sampled_strs = tokenizer.batch_decode(sampled, skip_special_tokens=True)
        all_sampled_outputs.extend(sampled_strs)

    return all_sampled_outputs

"""**7. GRPO Training: PUT ALL TOGETHER**"""

from drgrpo_grader import r1_zero_reward_fn
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_name = "Qwen/Qwen2.5-0.5B"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

policy_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

# Load dataset
train_data_path = "train_1k.jsonl"
dataset = MathRLDataset(train_data_path, tokenizer)
dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

from copy import deepcopy

def grpo_training_loop(
    policy_model: torch.nn.Module,
    dataloader: DataLoader,
    toeknizer: AutoTokenizer,
    reward_fn = r1_zero_reward_fn,
    device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    max_epochs: int = 1,
    n_grpo_steps: int = 3,
    G: int = 4,
    cliprange: float = 0.2,
    advantage_eps: float = 1e-8,
    normalize_by_std: bool = True,
    gradient_accumulation_steps: int = 1,
    loss_type: str = "grpo_clip",
):
    optimizer = torch.optim.Adam(policy_model.parameters(), lr=1e-5)
    policy_model.to(device)
    policy_model.train()

    for epoch in range(max_epochs):
        for step, batch in enumerate(dataloader):
            if step >= n_grpo_steps:
                break

            prompt_strs = batch["prompt_str"]
            groundtruth_strs = batch["groundtruth_str"]

            # Step 1: Old policy snapshot
            policy_model_old = deepcopy(policy_model)
            policy_model_old.eval()
            policy_model_old.to(device)

            # Step 2: Sample G outputs per prompt
            all_sampled_outputs = sample_g_outputs(
                policy_model=policy_model,
                tokenizer=tokenizer,
                prompt_strs=prompt_strs,
                G=G,
                device=device,
            )

            # Step 3: Compute group-normalized advantages
            repeated_groundtruths = groundtruth_strs * G

            advantages, raw_rewards, stats = compute_group_normalized_rewards(
                reward_fn=reward_fn,
                rollout_responses=all_sampled_outputs,
                repeated_ground_truths=repeated_groundtruths,
                group_size=G,
                advantage_eps=advantage_eps,
                normalize_by_std=normalize_by_std,
            )
            advantages = advantages.to(device)

            # Group outputs by G for training
            grouped_outputs = [
                all_sampled_outputs[i * len(prompt_strs):(i + 1) * len(prompt_strs)]
                for i in range(G)
            ]

            # Step 4: Training steps per rollout batch
            for g in range(G):
                output_strs = grouped_outputs[g]

                tokenized = tokenize_prompt_and_output(prompt_strs, output_strs, tokenizer)
                input_ids = tokenized["input_ids"].to(device)
                labels = tokenized["labels"].to(device)
                response_mask = tokenized["response_mask"].to(device)

                policy_log_probs = get_response_log_probs(
                    model=policy_model,
                    input_ids=input_ids,
                    labels=labels,
                    return_token_entropy=False,
                )["log_probs"]

                with torch.no_grad():
                    old_log_probs = get_response_log_probs(
                    model=policy_model_old,
                    input_ids=input_ids,
                    labels=labels,
                    return_token_entropy=False,
                )["log_probs"]

                start_idx = g * len(prompt_strs)
                end_idx = (g + 1) * len(prompt_strs)
                group_advantages = advantages[start_idx:end_idx].view(-1, 1)  # (batch_size, 1)

                loss, loss_info = grpo_microbatch_train_step(
                    policy_log_probs=policy_log_probs,
                    response_mask=response_mask,
                    gradient_accumulation_steps=gradient_accumulation_steps,
                    loss_type=loss_type,
                    advantages=group_advantages,
                    old_log_probs=old_log_probs,
                    cliprange=cliprange,
                )

                entropy = compute_entropy(policy_log_probs).mean().item()
                print(
                      f"[Epoch {epoch+1} Step {step+1} Group {g}] "
                      f"Loss: {loss.item():.4f} | Entropy: {entropy:.4f} | Advantage mean: {group_advantages.mean().item():.4f}"
                )

                # Optimizer step and zero_grad if not using gradient accumulation
                if gradient_accumulation_steps == 1:
                    optimizer.step()
                    optimizer.zero_grad()

            # If using gradient accumulation, step optimizer after n accumulation steps
            if gradient_accumulation_steps > 1 and (step + 1) % gradient_accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()

    # Save the trained model
    # torch.save(policy_model.state_dict(), "grpo_trained_model.pt")
    policy_model.save_pretrained("grpo_trained_model_dir")
    tokenizer.save_pretrained("grpo_trained_tokenizer_dir")
    print("Training complete. Model saved to grpo_trained_model_dir")

grpo_training_loop(policy_model, dataloader, tokenizer, n_grpo_steps=100)