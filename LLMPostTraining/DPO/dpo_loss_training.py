# -*- coding: utf-8 -*-
"""dpo_loss_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LVyBcKNr5PSGqFdzgwHWOcH5Luk8D3BT
"""

import torch
import torch.nn.functional as F
from transformers import LlamaForCausalLM, LlamaConfig

torch.manual_seed(42)

config = LlamaConfig(
    vocab_size=32,
    hidden_size=256,
    intermediate_size=512,
    num_hidden_layers=2,
    num_key_value_heads=4,
    num_attention_heads=4,
)

ref_model = LlamaForCausalLM(config)
ref_model.eval()

model = LlamaForCausalLM(config)
print(model.state_dict().keys())
print(model.lm_head)

"""## Create Preference data
###  Chosen :   [Prompt Token,  Response Chosen Token]
###  Rejected :   [Prompt Token,  Response Rejected Token]
"""

prompt_len = 6
response_len = 4

prompt_chosen = torch.tensor([[5, 8, 9, 10, 5, 3,   16, 29, 18, 17]], dtype=torch.int64)
prompt_rejected = torch.tensor([[5, 8, 9, 10, 5, 3,   26, 14, 31, 0]], dtype=torch.int64)
attention_mask = torch.tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1]], dtype=torch.bool)

x_chosen = {"input_ids": prompt_chosen, "attention_mask": attention_mask}
x_rejected = {"input_ids": prompt_rejected, "attention_mask": attention_mask}

def get_probs(logits, labels):
  """
    logits: [batch_size, seq_len, vocab_size]
    labels: [batch_size, seq_len]
    return: [batch_size, seq_len]
  """
  log_probs = F.log_softmax(logits, dim=-1)
  per_token_logprobs = torch.gather(log_probs, dim=2, index=labels.unsqueeze(2))

  return per_token_logprobs.squeeze(-1)

"""### calculate: ref/model, chosen/rejected,  logtis/prob value"""

with torch.no_grad():
  logits_chosen_ref = ref_model(**x_chosen).logits
  logits_rejected_ref = ref_model(**x_rejected).logits

  logits_chosen = model(**x_chosen).logits
  logits_rejected = model(**x_rejected).logits

# log p_{ref}(y1|x)
probs_chosen_ref = get_probs(logits_chosen_ref, prompt_chosen)
# log p_{ref}(y2|x)
probs_rejected_ref = get_probs(logits_rejected_ref, prompt_rejected)
# log p(y1|x)
probs_chosen = get_probs(logits_chosen, prompt_chosen)
# log p(y2|x)
probs_rejected = get_probs(logits_rejected, prompt_rejected)

probs_chosen_ref.shape

"""## DPO Loss
### p(y1 > y2) = sigmoid(r(x, y1) - r(x, y2))
### r(x, y1) - r(x, y2) = beta * (log(pi(y1)/pi(y2)) - log(p_{ref}(y1)/p_{ref}(y2))
### log sigmoid: L = -(log p(y1 > y2) 1(y1 > y2) + log (1-p(y1>y2))
### 1(y2 > y1))
"""

pi_logratio = probs_chosen - probs_rejected
ref_logratio = probs_chosen_ref - probs_rejected_ref

logits = pi_logratio - ref_logratio

beta = 0.1
losses = -F.logsigmoid(beta * logits) * attention_mask
print(losses)

loss = losses.sum(-1) / attention_mask.sum()
print(loss)

"""## DPO Training"""

def dpo_forward(model, ref_model, x_chose, x_rejected, beta=0.1):
  with torch.no_grad():
    logits_chosen_ref = ref_model(**x_chosen).logits
    logits_rejected_ref = ref_model(**x_rejected).logits

  logits_chosen = model(**x_chosen).logits
  logits_rejected = model(**x_rejected).logits

  # log p_{ref}(y1|x)
  probs_chosen_ref = get_probs(logits_chosen_ref, prompt_chosen)
  # log p_{ref}(y2|x)
  probs_rejected_ref = get_probs(logits_rejected_ref, prompt_rejected)
  # log p(y1|x)
  probs_chosen = get_probs(logits_chosen, prompt_chosen)
  # log p(y2|x)
  probs_rejected = get_probs(logits_rejected, prompt_rejected)

  pi_logratio = probs_chosen - probs_rejected
  ref_logratio = probs_chosen_ref - probs_rejected_ref

  logits = pi_logratio - ref_logratio

  losses = -F.logsigmoid(beta * logits) * attention_mask
  loss = losses.sum(-1) / attention_mask.sum()

  return logits, loss

def train_DPO(model, ref_model, beta=0.1, lr = 0.01, epoches=100):
  optimizer = torch.optim.Adam(model.parameters(), lr=lr)

  logistic_prob = [] # DPO beta( log(pi_w/pi_ref_w)  - log(pi_l/pi_ref_l))
  loss_record = [] # DPO loss
  for epoch in range(epoches):

    optimizer.zero_grad()

    logits, loss = dpo_forward(model, ref_model, x_chosen, x_rejected, beta=beta)
    loss.backward()
    optimizer.step()

    loss_record.append(loss.item())
    logistic_prob.append(torch.sigmoid(beta * logits)[:, -1].item())

    print(f'step {epoch}, loss:{loss.item()}, log_prob:{logistic_prob[-1]}')


  return loss_record, logistic_prob

loss, prob = train_DPO(model, ref_model, beta=0.1, lr = 0.01, epoches=100)

"""## XPO (DPO, IPO) training"""

def xpo_forward(model, ref_model, x_chose, x_rejected, loss_type='DPO', beta=0.1):
  with torch.no_grad():
    logits_chosen_ref = ref_model(**x_chosen).logits
    logits_rejected_ref = ref_model(**x_rejected).logits

  logits_chosen = model(**x_chosen).logits
  logits_rejected = model(**x_rejected).logits

  # log p_{ref}(y1|x)
  probs_chosen_ref = get_probs(logits_chosen_ref, prompt_chosen)
  # log p_{ref}(y2|x)
  probs_rejected_ref = get_probs(logits_rejected_ref, prompt_rejected)
  # log p(y1|x)
  probs_chosen = get_probs(logits_chosen, prompt_chosen)
  # log p(y2|x)
  probs_rejected = get_probs(logits_rejected, prompt_rejected)

  pi_logratio = probs_chosen - probs_rejected
  ref_logratio = probs_chosen_ref - probs_rejected_ref

  logits = pi_logratio - ref_logratio

  if loss_type == 'DPO':
    losses = -F.logsigmoid(beta * logits) * attention_mask

  elif  loss_type == 'IPO':
    losses = torch.square(logits - 1.0/(2*beta)) * attention_mask

  loss = losses.sum(-1) / attention_mask.sum()
  return logits, loss

def train_XPO(model, ref_model, loss_type = 'DPO', beta=0.1, lr = 0.01, epoches=100):
  optimizer = torch.optim.Adam(model.parameters(), lr=lr)

  logistic_prob = [] # DPO beta( log(pi_w/pi_ref_w)  - log(pi_l/pi_ref_l))
  loss_record = [] # DPO loss
  for epoch in range(epoches):

    optimizer.zero_grad()

    logits, loss = xpo_forward(model, ref_model, x_chosen, x_rejected, loss_type=loss_type, beta=beta)
    loss.backward()
    optimizer.step()

    loss_record.append(loss.item())
    logistic_prob.append(torch.sigmoid(beta * logits)[:, -1].item())

    print(f'step {epoch}, loss:{loss.item()}, log_prob:{logistic_prob[-1]}')

  return loss_record, logistic_prob

loss, prob = train_XPO(model, ref_model, loss_type='IPO', beta=0.1, lr = 0.01, epoches=100)