# -*- coding: utf-8 -*-
"""DPO Training with LoRA for Qwen2.5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XGzjmEXsWWV9-LIf6HAiPsLcaP6N5xbr

*More details in this article: [DPO Full Training vs. DPO with LoRA: How Good is LoRA for DPO Training?](https://newsletter.kaitchup.com/p/dpo-full-training-vs-dpo-with-lora)*


This notebook shows how to run DPO training with only one base model and two LoRA adapters: one for the reference and one for the policy. It uses Qwen2.5 1.5B for example.

The notebook requires a 24 GB GPU (Ampere or more recent).
"""

# !pip install --upgrade transformers bitsandbytes peft accelerate datasets trl flash_attn

import torch, os, multiprocessing
from datasets import load_dataset
from peft import LoraConfig, PeftModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    set_seed
)
from trl import DPOTrainer, DPOConfig

set_seed(1234)

model_name = "Qwen/Qwen2.5-1.5B"

#SFT Adapter
sft_adapter = "kaitchup/Qwen2.5-1.5B-SFT-UltraChat" #location of your SFT adapter

compute_dtype = torch.bfloat16

#If you have troubles with FlashAttention, use 'sdpa' instead
# attn_implementation = 'flash_attention_2'
attn_implementation = 'sdpa'



bs = 4 #Batch size per device (training and validation)
gas = 8 #Gradient accumulation steps
mseqlen = 1024 #Maximum sequence length
lr = 1e-6 #Learning rate

lora_alpha = 16
lora_dropout = 0.0
lora_r = 16

output_dir = "/workspace/DPO_LoRA_RUN/"

#Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = "<|image_pad|>"
tokenizer.pad_token_id = 151655
tokenizer.padding_side = 'left'

import torch
print("CUDA available:", torch.cuda.is_available())
print("Device count:", torch.cuda.device_count())
print("Current device:", torch.cuda.current_device())
print("Device name:", torch.cuda.get_device_name(0))

"""DPO expects a dataset with:
* a prompt
* a chosen answer to this prompt
* a rejected answer to this prompt
"""

ds = load_dataset("mlabonne/orpo-dpo-mix-40k", split="train").train_test_split(test_size=0.01)
ds_train = ds['train']
ds_test = ds['test']

#Add the EOS token
def process(row):
    prompt_messages = tokenizer.apply_chat_template([row["chosen"][0]], tokenize=False)
    # Now we extract the final turn to define chosen/rejected responses
    chosen_messages = tokenizer.apply_chat_template(row["chosen"][1:], tokenize=False)+tokenizer.eos_token
    rejected_messages = tokenizer.apply_chat_template(row["rejected"][1:], tokenize=False)+tokenizer.eos_token
    row["prompt"] = prompt_messages
    row["chosen"] = chosen_messages
    row["rejected"] = rejected_messages
    return row

ds_train = ds_train.map(
    process,
    num_proc= multiprocessing.cpu_count(),
    load_from_cache_file=False,
)

ds_test = ds_test.map(
    process,
    num_proc= multiprocessing.cpu_count(),
    load_from_cache_file=False,
)


# load base model
model = AutoModelForCausalLM.from_pretrained(
      model_name, device_map={"": 0}, torch_dtype=compute_dtype, attn_implementation=attn_implementation)

model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant':True})

"""The following cell displays some warnings that you can safely ignore."""

# Load the policy adapter (this one is trainable)
model = PeftModel.from_pretrained(model, sft_adapter, is_trainable=True, adapter_name="DPO")

# Load the reference adapter (must be named "reference")
model.load_adapter(sft_adapter, adapter_name="reference")

training_arguments = DPOConfig(
        output_dir=output_dir,
        eval_strategy="steps",
        do_eval=True,
        optim="paged_adamw_8bit",
        per_device_train_batch_size=bs,
        gradient_accumulation_steps=gas,
        per_device_eval_batch_size=bs,
        log_level="debug",
        save_strategy="steps",
        save_steps=200,
        logging_steps=25,
        learning_rate=lr,
        bf16 = True,
        beta = 0.1,
        eval_steps=25,
        num_train_epochs=0.1,
        warmup_ratio=0.1,
        lr_scheduler_type="linear",
        max_length=mseqlen,
        max_prompt_length=mseqlen,
        model_adapter_name="DPO",
        ref_adapter_name="reference",
        dataset_num_proc=multiprocessing.cpu_count(),
)

trainer = DPOTrainer(
    model,
    args=training_arguments,
    train_dataset=ds_train,
    eval_dataset=ds_test,
    processing_class=tokenizer,
)

"""The most important metrics in the training logs are Rewards/accuracies and Rewards/margins which should be increasing. Note that DPO is very sensitive to the learning rate. I recommending trying different values from 5e-7 to 5e-5.

"""

trainer_ = trainer.train()