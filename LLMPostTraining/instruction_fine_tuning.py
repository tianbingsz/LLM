# -*- coding: utf-8 -*-
"""Copy of sft_train_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kEhDoe4lK3VKUYETiAUjePB11zgNOpel

# Supervised Fine-Tuning with GPT2 writing from Scratch
"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch.nn.functional as F

"""## LOAD TOKENIZER and MODEL"""

model_name='Qwen/Qwen2.5-0.5B'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map='auto',
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'left'

"""## Alphaca dataset

"""

!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json

ALACAPA_PROMPT = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:
{response}
"""

import json
import random
import numpy as np

class InstructionDataset(Dataset):
    def __init__(self, tokenizer, file_path='alpaca_data.json', num_samples=32, seq_length=1024):
        super().__init__()
        self.tokenizer = tokenizer
        self.num_samples = num_samples
        self.seq_length = seq_length
        self.end_of_text_token = tokenizer.eos_token

        formatted_texts = self._load_dataset(file_path)
        self._tokenize_dataset(formatted_texts)


    def _load_dataset(self, file_path='alpaca_data.json'):
        # Load and parse dataset
        with open(file_path, 'r') as f:
            data = json.load(f)

        indices = np.random.randint(0, len(data), (self.num_samples, ))
        # Format all (prompt, response) pairs
        formatted_texts = []
        for idx in indices:
            item = data[int(idx)]
            instruction = item.get("instruction", "")
            input_text = item.get("input", "")
            response = item.get("output", "")
            formatted_text = ALACAPA_PROMPT.format(
                instruction=instruction,
                input=input_text,
                response=response,
            )
            formatted_texts.append(formatted_text)

        return formatted_texts


    def _tokenize_dataset(self, formatted_texts):
        # Tokenize and concatenate all into a single stream
        self.tokenized_texts = []
        for example in formatted_texts:
            encoded_text = self.tokenizer(
                example,
                truncation=True,
                padding="max_length",
                max_length=self.seq_length,
                return_tensors="pt",
            )
            input_ids = encoded_text.input_ids.squeeze(0)
            attention_mask = encoded_text.attention_mask.squeeze(0)
            labels = input_ids.clone() # For causal LM, labels are input_ids

            self.tokenized_texts.append({
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "labels": labels,
            })

        print(f"Total tokens: {len(self.tokenized_texts)}")

    def __len__(self):
        return len(self.tokenized_texts)


    def __getitem__(self, idx):
        return self.tokenized_texts[idx]

"""### Create dataset and dataloader

"""

num_samples = 10240
max_len = 128
batch_size = 32

dataset = InstructionDataset(tokenizer, num_samples=num_samples, seq_length=max_len)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

"""## Train SFT"""

def forward(model : AutoModelForCausalLM, input_ids, labels, attention_mask=None):
    """
        input_ids: (B, T)
        attention_mask: (B, T)
        labels: (B, T)
    """
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    logits = outputs.logits
    batch_size, seq_len, vocab_size = logits.size()
    shift_logits = logits[:, :-1, :].contiguous() # (B, T-1, V)
    shift_labels = labels[:, 1:].contiguous() # (B, T-1), next token prediction

    # loss
    if attention_mask is None:
        loss = F.cross_entropy(
            shift_logits.view(-1, vocab_size), # (B * (T-1), V)
            shift_labels.view(-1) # (B * (T-1), )
        )
        return loss # scalar

    # attention_mask is not None:
    loss = F.cross_entropy(
        shift_logits.view(-1, vocab_size),
        shift_labels.view(-1),
        reduce='none',
    ) # loss per token, (B * (T-1), )
    shift_mask = attention_mask[:, 1:].contiguous() # (B, T-1)
    loss = (loss * shift_mask.view(-1)).sum() / shift_mask.sum() # scalar
    return loss

def sft_training_loop(model, dataloader, lr=5e-5, max_epoches=3, device=device):
    model.train()
    model.to(device)

    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)
    for epoch in range(max_epoches):
        for step, batch in enumerate(dataloader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            loss = forward(model=model, input_ids=input_ids, labels=labels, attention_mask=attention_mask)
            loss.backward()
            optimizer.step()

            print(f"[Epoch {epoch+1} Step {step+1}] Loss: {loss.item():.4f}")

def micro_training_step(model : AutoModelForCausalLM, gradient_accumulation_steps, input_ids, labels, attention_mask=None):
    """
    Perform a single microbatch train step for SFT using cross-entropy loss.
    microstep is able to enlarge the training batch size without introducing memory cost

    Args:
        gradient_accumulation_steps (int): Number of microbatches per optimizer step.

    Returns:
        loss (scalar)
    """
    model.train()
    loss = forward(model=model, input_ids=input_ids, labels=labels, attention_mask=attention_mask)
    loss /= gradient_accumulation_steps

    return loss

def sft_full_training_loop(model, dataloader, gradient_accumulation_steps=4, lr=5e-5, max_epoches=3, device=device):
    """
        Fine-tuning base model for max_epoches with instruction following data, with each micro training step
    """
    model.train()
    model.to(device)
    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)

    for epoch in range(max_epoches):

        optimizer.zero_grad()
        loss_accum = 0
        for step, batch in enumerate(dataloader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            loss = micro_training_step(model, gradient_accumulation_steps, input_ids, labels, attention_mask)
            loss_accum += loss.item()

            # reset optimizer for each gradient_accumulation_steps steps
            if (step+1) % gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
                print(f"Epoch: {epoch + 1}, Step: {step+1}, loss: {loss_accum:.4f}")
                loss_accum = 0

sft_full_training_loop(model=model, dataloader=dataloader, gradient_accumulation_steps=4, max_epoches=1, device=device)

sft_training_loop(model=model, dataloader=dataloader, max_epoches=1, device=device)

model.save_pretrained("path/to/save_dir")
tokenizer.save_pretrained("path/to/save_dir")