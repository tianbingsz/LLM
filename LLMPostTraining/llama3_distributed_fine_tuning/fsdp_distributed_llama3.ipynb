{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3731bd-ad8f-4905-8f02-de0dcb4088ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fsdp-qlora-distributed-llama3.ipynb\n",
    "# Efficiently scale distributed training Llama 3, PyTorch FSDP and Q-Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d523a-b4d1-435f-85d8-a612a377590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39eaac6-9e95-4807-b1f1-72b4019a6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m pip install --upgrade pip\n",
    "# pip install torch==2.2.2+cu118  torchvision torchaudio -f https://download.pytorch.org/whl/cu118/torch_stable.html\n",
    "# pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe9e9c0-8119-4095-b16e-28451238dd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743282e1-332d-401a-b77b-27d07f48f178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu118\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c81c45-f4a5-47ce-be14-26673d1fb19b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81e1c66-f4e5-4d61-81d2-54c19c24b6de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/tianbing_xu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47383b8-7ffc-4fed-9284-85790e44f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b53540-050d-4151-82b6-962120fcd84f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365f8f0f7eb646f182d74f12663e8877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8334583c29b7473180501bfde350eee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "784047"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "      sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "      return sample\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove,batched=False)\n",
    "\n",
    "# Filter out conversations which are corrupted with wrong turns, keep which have even number of turns after adding system message\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "\n",
    "# save datasets to disk \n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2b07a-a7cb-4256-80b4-6c8ef19529d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fine-tune the LLM with PyTorch FSDP, Q-Lora and SDPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4bffa4e-799f-4c48-bd1b-60f7a32b34bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id: \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "dataset_path: \".\"                      # path to dataset\n",
    "max_seq_len:  3072 # 2048              # max sequence length for model and packing of the dataset\n",
    "# training parameters\n",
    "output_dir: \"./llama-3-8b-hf-no-robot\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37003de5-d490-4558-868a-76de15b96341",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-26 19:37:21,876] torch.distributed.run: [WARNING] \n",
      "[2024-04-26 19:37:21,876] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-04-26 19:37:21,876] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-04-26 19:37:21,876] torch.distributed.run: [WARNING] *****************************************\n",
      "2024-04-26 19:37:26.521212: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 19:37:26.521217: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 19:37:26.521219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 19:37:26.521231: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 19:37:27.329810: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-04-26 19:37:27.329840: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-04-26 19:37:27.329867: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-04-26 19:37:27.329923: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-04-26 19:37:27.329938: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-04-26 19:37:27.329946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-26 19:37:27.329951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-26 19:37:27.329975: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-04-26 19:37:27.329997: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-26 19:37:27.331571: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-04-26 19:37:27.331671: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-04-26 19:37:27.331691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Generating train split: 9485 examples [00:00, 109805.81 examples/s]\n",
      "Generating train split: 500 examples [00:00, 80313.73 examples/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|█████████████████████████| 9485/9485 [00:01<00:00, 9337.55 examples/s]\n",
      "Map: 100%|█████████████████████████| 9485/9485 [00:01<00:00, 8672.12 examples/s]\n",
      "Map: 100%|█████████████████████████| 9485/9485 [00:01<00:00, 8652.87 examples/s]\n",
      "Map: 100%|█████████████████████████| 9485/9485 [00:01<00:00, 8871.97 examples/s]\n",
      "Map: 100%|██████████████████████████| 500/500 [00:00<00:00, 11014.28 examples/s]\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>\n",
      "\n",
      "\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>\n",
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.75s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.77s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.76s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.78s/it]\n",
      "Generating train split: 5904 examples [00:01, 3450.04 examples/s]\n",
      "Generating train split: 315 examples [00:00, 3449.63 examples/s]\n",
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\n",
      "{'loss': 2.1241, 'grad_norm': 0.37109375, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8493, 'grad_norm': 0.2021484375, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8375, 'grad_norm': 0.2255859375, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8546, 'grad_norm': 0.20703125, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8583, 'grad_norm': 0.197265625, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8006, 'grad_norm': 0.22265625, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8117, 'grad_norm': 0.24609375, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8523, 'grad_norm': 0.158203125, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7759, 'grad_norm': 0.236328125, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.7699, 'grad_norm': 0.279296875, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.8289, 'grad_norm': 0.234375, 'learning_rate': 0.0002, 'epoch': 0.15} \n",
      "{'loss': 1.6955, 'grad_norm': 0.18359375, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.8156, 'grad_norm': 0.20703125, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 1.8086, 'grad_norm': 0.173828125, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.7248, 'grad_norm': 0.2236328125, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.7302, 'grad_norm': 0.228515625, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.7853, 'grad_norm': 0.1953125, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.7959, 'grad_norm': 0.16796875, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.8133, 'grad_norm': 0.1845703125, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.7541, 'grad_norm': 0.162109375, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.7756, 'grad_norm': 0.220703125, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.8267, 'grad_norm': 0.2333984375, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.8068, 'grad_norm': 0.365234375, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.7175, 'grad_norm': 0.2138671875, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.7642, 'grad_norm': 0.1884765625, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.8252, 'grad_norm': 0.193359375, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.7927, 'grad_norm': 0.2001953125, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.7634, 'grad_norm': 0.25, 'learning_rate': 0.0002, 'epoch': 0.38}     \n",
      "{'loss': 1.7737, 'grad_norm': 0.1728515625, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.7253, 'grad_norm': 0.1875, 'learning_rate': 0.0002, 'epoch': 0.41}   \n",
      "{'loss': 1.8132, 'grad_norm': 0.169921875, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.7604, 'grad_norm': 0.181640625, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.8199, 'grad_norm': 0.197265625, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.7326, 'grad_norm': 0.2294921875, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.7327, 'grad_norm': 0.2041015625, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.7984, 'grad_norm': 0.1767578125, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.838, 'grad_norm': 0.275390625, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.7631, 'grad_norm': 0.2060546875, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.7941, 'grad_norm': 0.1884765625, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.7544, 'grad_norm': 0.2158203125, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.7805, 'grad_norm': 0.1748046875, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.7084, 'grad_norm': 0.287109375, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.7635, 'grad_norm': 0.1728515625, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8001, 'grad_norm': 0.1708984375, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.7154, 'grad_norm': 0.21875, 'learning_rate': 0.0002, 'epoch': 0.85}  \n",
      "{'loss': 1.7687, 'grad_norm': 0.1845703125, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.7563, 'grad_norm': 0.20703125, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.7744, 'grad_norm': 0.1650390625, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.707, 'grad_norm': 0.1591796875, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.7512, 'grad_norm': 0.2373046875, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.8208, 'grad_norm': 0.1767578125, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.74, 'grad_norm': 0.1728515625, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.7077, 'grad_norm': 0.189453125, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.6888, 'grad_norm': 0.171875, 'learning_rate': 0.0002, 'epoch': 0.98} \n",
      "{'loss': 1.8021, 'grad_norm': 0.2353515625, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "100%|███████████████████████████████████████| 738/738 [1:23:57<00:00,  6.79s/it]\n",
      "  0%|                                                    | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█                                           | 2/79 [00:00<00:38,  2.01it/s]\u001b[A\n",
      "  4%|█▋                                          | 3/79 [00:01<00:53,  1.42it/s]\u001b[A\n",
      "  5%|██▏                                         | 4/79 [00:02<01:01,  1.23it/s]\u001b[A\n",
      "  6%|██▊                                         | 5/79 [00:03<01:04,  1.15it/s]\u001b[A\n",
      "  8%|███▎                                        | 6/79 [00:04<01:06,  1.10it/s]\u001b[A\n",
      "  9%|███▉                                        | 7/79 [00:05<01:07,  1.07it/s]\u001b[A\n",
      " 10%|████▍                                       | 8/79 [00:06<01:07,  1.04it/s]\u001b[A\n",
      " 11%|█████                                       | 9/79 [00:07<01:07,  1.03it/s]\u001b[A\n",
      " 13%|█████▍                                     | 10/79 [00:08<01:07,  1.02it/s]\u001b[A\n",
      " 14%|█████▉                                     | 11/79 [00:09<01:07,  1.01it/s]\u001b[A\n",
      " 15%|██████▌                                    | 12/79 [00:10<01:06,  1.01it/s]\u001b[A\n",
      " 16%|███████                                    | 13/79 [00:11<01:05,  1.00it/s]\u001b[A\n",
      " 18%|███████▌                                   | 14/79 [00:12<01:04,  1.00it/s]\u001b[A\n",
      " 19%|████████▏                                  | 15/79 [00:13<01:03,  1.01it/s]\u001b[A\n",
      " 20%|████████▋                                  | 16/79 [00:14<01:02,  1.01it/s]\u001b[A\n",
      " 22%|█████████▎                                 | 17/79 [00:15<01:01,  1.01it/s]\u001b[A\n",
      " 23%|█████████▊                                 | 18/79 [00:16<01:00,  1.01it/s]\u001b[A\n",
      " 24%|██████████▎                                | 19/79 [00:17<00:59,  1.00it/s]\u001b[A\n",
      " 25%|██████████▉                                | 20/79 [00:18<00:59,  1.00s/it]\u001b[A\n",
      " 27%|███████████▍                               | 21/79 [00:19<00:58,  1.01s/it]\u001b[A\n",
      " 28%|███████████▉                               | 22/79 [00:20<00:57,  1.01s/it]\u001b[A\n",
      " 29%|████████████▌                              | 23/79 [00:22<00:56,  1.02s/it]\u001b[A\n",
      " 30%|█████████████                              | 24/79 [00:23<00:55,  1.02s/it]\u001b[A\n",
      " 32%|█████████████▌                             | 25/79 [00:24<00:54,  1.01s/it]\u001b[A\n",
      " 33%|██████████████▏                            | 26/79 [00:25<00:53,  1.01s/it]\u001b[A\n",
      " 34%|██████████████▋                            | 27/79 [00:26<00:52,  1.01s/it]\u001b[A\n",
      " 35%|███████████████▏                           | 28/79 [00:27<00:51,  1.01s/it]\u001b[A\n",
      " 37%|███████████████▊                           | 29/79 [00:28<00:50,  1.01s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 30/79 [00:29<00:49,  1.01s/it]\u001b[A\n",
      " 39%|████████████████▊                          | 31/79 [00:30<00:48,  1.00s/it]\u001b[A\n",
      " 41%|█████████████████▍                         | 32/79 [00:31<00:47,  1.00s/it]\u001b[A\n",
      " 42%|█████████████████▉                         | 33/79 [00:32<00:45,  1.00it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 34/79 [00:33<00:45,  1.00s/it]\u001b[A\n",
      " 44%|███████████████████                        | 35/79 [00:34<00:43,  1.00it/s]\u001b[A\n",
      " 46%|███████████████████▌                       | 36/79 [00:35<00:42,  1.00it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 37/79 [00:36<00:42,  1.00s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 38/79 [00:37<00:41,  1.00s/it]\u001b[A\n",
      " 49%|█████████████████████▏                     | 39/79 [00:38<00:39,  1.00it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 40/79 [00:39<00:38,  1.00it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 41/79 [00:40<00:37,  1.01it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 42/79 [00:41<00:36,  1.01it/s]\u001b[A\n",
      " 54%|███████████████████████▍                   | 43/79 [00:41<00:35,  1.01it/s]\u001b[A\n",
      " 56%|███████████████████████▉                   | 44/79 [00:42<00:34,  1.01it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 45/79 [00:43<00:33,  1.01it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 46/79 [00:44<00:32,  1.01it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 47/79 [00:45<00:31,  1.01it/s]\u001b[A\n",
      " 61%|██████████████████████████▏                | 48/79 [00:46<00:30,  1.01it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 49/79 [00:47<00:29,  1.01it/s]\u001b[A\n",
      " 63%|███████████████████████████▏               | 50/79 [00:48<00:28,  1.01it/s]\u001b[A\n",
      " 65%|███████████████████████████▊               | 51/79 [00:49<00:27,  1.01it/s]\u001b[A\n",
      " 66%|████████████████████████████▎              | 52/79 [00:50<00:26,  1.01it/s]\u001b[A\n",
      " 67%|████████████████████████████▊              | 53/79 [00:51<00:25,  1.01it/s]\u001b[A\n",
      " 68%|█████████████████████████████▍             | 54/79 [00:52<00:24,  1.01it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 55/79 [00:53<00:23,  1.01it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 56/79 [00:54<00:22,  1.01it/s]\u001b[A\n",
      " 72%|███████████████████████████████            | 57/79 [00:55<00:21,  1.01it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 58/79 [00:56<00:20,  1.01it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 59/79 [00:57<00:19,  1.00it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 60/79 [00:58<00:18,  1.00it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▏         | 61/79 [00:59<00:17,  1.00it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 62/79 [01:00<00:16,  1.01it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 63/79 [01:01<00:15,  1.01it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 64/79 [01:02<00:14,  1.00it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 65/79 [01:03<00:13,  1.00it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 66/79 [01:04<00:12,  1.00it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 67/79 [01:05<00:11,  1.01it/s]\u001b[A\n",
      " 86%|█████████████████████████████████████      | 68/79 [01:06<00:10,  1.01it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▌     | 69/79 [01:07<00:09,  1.01it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████     | 70/79 [01:08<00:08,  1.01it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 71/79 [01:09<00:07,  1.01it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 72/79 [01:10<00:06,  1.00it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 73/79 [01:11<00:06,  1.00s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 74/79 [01:12<00:05,  1.00s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 75/79 [01:13<00:04,  1.01s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 76/79 [01:14<00:03,  1.01s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▉ | 77/79 [01:15<00:02,  1.01s/it]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 78/79 [01:16<00:01,  1.01s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7915854454040527, 'eval_runtime': 79.1279, 'eval_samples_per_second': 3.981, 'eval_steps_per_second': 0.998, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████| 738/738 [1:25:16<00:00,  6.79s/it]\n",
      "100%|███████████████████████████████████████████| 79/79 [01:17<00:00,  1.00s/it]\u001b[A\n",
      "                                                                                \u001b[A[rank0]:[2024-04-26 21:03:17,720] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.2476888790006342, 'preprocessing_with_comm': 0.00329463200068858, 'state_converting': 0.20913432799898146, <Type.ALL: 'all'>: 0.4713482999995904})\n",
      "{'train_runtime': 5128.393, 'train_samples_per_second': 1.151, 'train_steps_per_second': 0.144, 'train_loss': 1.7808038445346077, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████| 738/738 [1:25:28<00:00,  6.95s/it]\n"
     ]
    }
   ],
   "source": [
    "!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 run_fsdp_qlora.py --config llama_3_8b_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6c8b4-6345-40cd-b6c5-df1cd864ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Test Model and run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2efd6bea-4cb9-4706-8dec-5432e637728b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d62bf0e590490db260061c31131bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "peft_model_id = \"./llama-3-8b-hf-no-robot\"\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  torch_dtype=torch.float16,\n",
    "  quantization_config= {\"load_in_4bit\": True},\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897f4f9-ce85-4fd6-ac74-94e1c7a1760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load our test dataset try to generate an instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b28d6763-d717-4421-8f44-845770853681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f6ac2a2e2f436d8870a5682fc1a58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/home/default_user/.conda/envs/user/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "2024-04-26 21:08:33.509122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 21:08:34.335776: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-04-26 21:08:34.335870: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-04-26 21:08:34.335879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Query:**\n",
      "Rewrite the article as study notes. Each section should include bullet points\n",
      "\n",
      "Glorious humanoids with gilded armors, beautiful wings, perfect faces, and an all-righteous nature that brings warmth, comfort, and solace — that’s how we picture angels. After all, that’s how we’ve depicted the majestic creatures throughout history in every piece of art and literature.\n",
      "\n",
      "Is this common description a portrayal of our reimagination, or does it have a striking resemblance to angels who rule heaven?\n",
      "\n",
      "According to the Bible, it would appear that we’re in the wrong. Unlike the gentle, human-like apparitions we’re familiar with, angels look strange, terrifying even. And it would appear that there’s a good reason behind it.\n",
      "\n",
      "\n",
      "Biblical Depiction of an Angel via reddit.com\n",
      "Conceptualizing Angels: A Brief History\n",
      "The word “angel” has a couple of origins. It is derived from the Greek word “angelos,” and the Greek word, in turn, comes from the Hebrew word “Mal’ akh,” which stands for messenger.\n",
      "\n",
      "\n",
      "When we’re visualizing an angel, we’re often thinking of Malakim, who acted on God’s behalf in the Old Testament to carry out his judgment and took on the role of messengers in the New Testament.\n",
      "\n",
      "Malakim are said to be the closest depiction of human beings. However, they are not mentioned in the Bible as beings with wings.\n",
      "\n",
      "Indeed, the earliest known Christian image of an angel, from the mid-third century, depicted them as human-like beings without wings.\n",
      "\n",
      "This image, however, changed in the late fourth century as artists reimagined angels with wings to represent their sublime nature, even though the scripture makes no mention of wings.\n",
      "\n",
      "\n",
      "Popular Angels and Their Biblical Forms\n",
      "Let’s take a look at some of the most well-known depictions of the angels we’re familiar with and how much they differ from their Biblical descriptions.\n",
      "\n",
      "Cherubim\n",
      "According to the Bible, cherubim are responsible for guarding the Garden of Eden — the Biblical terrestrial paradise — against humankind when Adam and Eve, the first humans, were driven out of the heavenly garden.\n",
      "\n",
      "The prophet Ezekiel’s vision of cherubim is depicted in the Book of Ezekiel in which they are portrayed as having four faces — one of an eagle, one a human, one an ox, and finally a lion.\n",
      "\n",
      "Cherubim have straight legs, four wings (one set covers their body while the other is used for flying), and bull hooves for feet.\n",
      "\n",
      "\n",
      "Cherubim – Art by Art by KelbremDusk\n",
      "Subscribe to the History Defined newsletter!\n",
      "Get updates on the latest posts and more from History Defined straight to your inbox.\n",
      "\n",
      "Your Email...\n",
      "SUBSCRIBE\n",
      "\n",
      "I consent to receiving emails and personalized ads.\n",
      "This portrayal is nothing like what we imagine them to be. Cherubim, commonly known as cherubs, are depicted as chubby, beautiful, naked children with wings in art, as represented by Renaissance sculptors who revived the ancient practice of putti.\n",
      "\n",
      "\n",
      "These depictions of Cherubim are often connected with divinity. These animal-human hybrids have also been associated with cupids, the diaper-clad chubby babies who are sometimes shown with a trumpet and arrow to symbolize romantic love.\n",
      "\n",
      "Some attribute this modern-day image to cupid-like Greek and Roman deities. On the other hand, the Biblical image is frequently attributed to cultural exchanges with ancient Babylonia, Egypt, and Syria, which explains their mixed appearance. \n",
      "\n",
      "\n",
      "Seraphim\n",
      "According to the Christian angel hierarchy, seraphim hold the highest rank. In art, the four-winged cherubim are painted in blue to symbolize the sky, while the six-winged seraphim are painted in red to symbolize fire.\n",
      "\n",
      "The Hebrew term “Saraph” means “venomous desert snake” and “Seraph” means “to burn.” These terms are the two main historical influences on Seraphim’s name.\n",
      "\n",
      "The Bible describes seraphim as having six wings, four of which are used to cover their heads and feet in front of God as a symbol of humility, and the remaining two are used to fly.\n",
      "\n",
      "They are second in ranking in the angel hierarchy, with their sheer presence emitting holiness. And unlike the ophanim and cherubim, seraphim are not guard-angels. \n",
      "\n",
      "\n",
      "Seraphim\n",
      "Seraphim are described by prophet Isaiah as angelic beings that continually worship God by surrounding God’s throne and singing “Holy, holy, holy is the Lord Almighty; the whole earth is full of his glory” in unison when God approaches.\n",
      "\n",
      "\n",
      "\n",
      "While they’re the epitome of helpfulness and forgiveness, their appearance instills fear in the prophet.\n",
      "\n",
      "Several historians suggest that the description of the Seraphim’s wings and flames may have been based on the connection with Egyptian imagery and description of the cobra.\n",
      "\n",
      "Ophanim\n",
      "“Their entire bodies, including their backs, hands, and wings, were full of eyes all around, as were their four wheels.” (Ezekiel 10:12)\n",
      "\n",
      "\n",
      "\n",
      "Ophanim, or “the wheels,” are one of the strangest, most bizarre beings referenced in Ezekiel’s vision. They’re portrayed as beings made of interlocking gold wheels, with every wheel adorned with numerous sets of eyes on the exterior.\n",
      "\n",
      "These wheels, however, do not change directions as the creatures move by floating in the skies. \n",
      "\n",
      "\n",
      "Ophanim\n",
      "The second book of Enoch refers to ophanim as “many-eyed ones.” They are sometimes also described as spheres or whirlwinds. They’re also classified as “thrones.”\n",
      "\n",
      "\n",
      "\n",
      "There’s an interesting theory that the ophanim are the wheels attached to God’s chariot. Others, like former NASA scientist Jose F. Blumrich, believe ophanim might have been what we would consider a UFO sighting today.\n",
      "\n",
      "While they’re not considered angels in the Bible, the Jewish angelic hierarchy considers them responsible for guarding God’s throne and being the angels closest to God. This notion also coincides with beliefs held in different traditions.\n",
      "\n",
      "\n",
      "**Original Answer:**\n",
      " Introduction\n",
      "\n",
      "How we picture angels:\n",
      "\n",
      "•Gilded armor\n",
      "•Beautiful wings\n",
      "•Glorious humanoids\n",
      "•All-righteous nature\n",
      "•Comfort, warmth, and solace\n",
      "\n",
      "Where majestic angels appear:\n",
      "\n",
      "•Every piece of art\n",
      "•Every piece of literature\n",
      "\n",
      "Biblical angels are:\n",
      "•Strange\n",
      "•Terrifying\n",
      "•Inhuman\n",
      "\n",
      "Origins of the word “angel”:\n",
      "•Derived from the Greek “Angelos”\n",
      "•Greek word is derived from the Hebrew “Mal’ akh”\n",
      "•“Mal’ akh” means “messenger”\n",
      "\n",
      "Types of Angels\n",
      "\n",
      "Malakim\n",
      "•Appeared in Old Testament to carry out God’s judgment\n",
      "•Appeared as messengers in the New Testament\n",
      "•Closest to human beings\n",
      "•Are never mentioned as having wings in the Bible\n",
      "•Third-century Christian image of an angel is the earliest known image\n",
      "•This image depicts angels as wingless humans\n",
      "•Images of wings were imagined by artists during the late 4th century\n",
      "\n",
      "Cherubim \n",
      "•Guarded the Garden of Eden against humans after Adam and Eve were removed\n",
      "•Depicted in Ezekiel\n",
      "•Have four faces\n",
      "•Faces are an ox, a lion, a human, and an eagle\n",
      "•Have straight legs\n",
      "•Have four wings\n",
      "•One set of wings covers the body\n",
      "•One is for flying\n",
      "•Have bull hooves instead of feet\n",
      "•Depictions in art as naked, chubby, beautiful winged children\n",
      "•This comes from Renaissance sculptors practicing putti\n",
      "•Cherubs in the art are associated with cupids\n",
      "•Modern image is similar to Greco-Roman deities\n",
      "•Biblical image attributed to ancient Egypt, Syria, and Babylon\n",
      "•Painted blue to symbolize the sky in art\n",
      "\n",
      "Seraphim\n",
      "•Highest-ranking angel in the Christian hierarchy\n",
      "•In art, painted red to symbolize fire\n",
      "•Six wings\n",
      "•Four wings cover feet and heads in humility when appearing before God\n",
      "•Two wings are used for flight\n",
      "•Influenced by the Hebrew “Saraph” meaning “venomous desert snake”\n",
      "•Influenced by the Hebrew “Seraph” meaning “to burn”\n",
      "•Their presence emits holiness\n",
      "•They are not guard angels\n",
      "•Described by Isaiah as continually worshiping God\n",
      "•They surround God’s throne and sing his glory in unison when God is near\n",
      "•Helpful\n",
      "•Forgiving\n",
      "•Instilled fear in the Prophet Isaiah\n",
      "•Some historians suggest a connection to Egyptian cobra imagery in the Seraphim’s flames and wings\n",
      "\n",
      "Ophanim\n",
      "•Described as four wheels made of eyes all around in Ezekiel\n",
      "•Described as “many-eyed ones” in Enoch\n",
      "•Made of interlocking gold wheels\n",
      "•Each wheel has many sets of eyes on its exterior\n",
      "•Float in the sky\n",
      "•Wheels do not change direction as they move\n",
      "•Sometimes described as whirlwinds\n",
      "•Sometimes described as spheres\n",
      "•Classified as Thrones\n",
      "•Some theorize they are the wheels of God’s chariot\n",
      "•NASA scientist Jose F. Blumrich believes they would be considered a UFO sighting today\n",
      "•Not considered angels in the Bible\n",
      "•Are considered angels in the Jewish hierarchy\n",
      "•Jewish hierarchy sees them as guardians of God’s throne\n",
      "•In Judaism, these angels are closest to God\n",
      "•This is supported by some other faiths and tradition\n",
      "\n",
      "**Generated Answer:**\n",
      "1. Angels are depicted as strange and terrifying in the Bible.\n",
      "2. The earliest known Christian image of an angel depicted them as human-like beings without wings.\n",
      "3. Cherubim are described as having four faces and four wings, one set of which covers their body while the other is used for flying.\n",
      "4. Cherubim are often depicted as chubby, beautiful, naked children with wings in art.\n",
      "5. The Biblical description of seraphim is as having six wings, four of which are used to cover their heads and feet in front of God as a symbol of humility, and the remaining two are used to fly.\n",
      "6. Ophanim are one of the strangest, most bizarre beings referenced in Ezekiel's vision. They're portrayed as beings made of interlocking gold wheels, with every wheel adorned with numerous sets of eyes on the exterior.\n",
      "7. The second book of Enoch refers to ophanim as \"many-eyed ones.\" They are sometimes also described as spheres or whirlwinds. They're also classified as \"thrones.\"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "\n",
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "messages = eval_dataset[rand_idx][\"messages\"][:2]\n",
    "\n",
    "# Test on sample \n",
    "input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id= tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "print(f\"**Query:**\\n{eval_dataset[rand_idx]['messages'][1]['content']}\\n\")\n",
    "print(f\"**Original Answer:**\\n{eval_dataset[rand_idx]['messages'][2]['content']}\\n\")\n",
    "print(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301a7e3-3988-457b-aebb-9491c35fc6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (User)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
