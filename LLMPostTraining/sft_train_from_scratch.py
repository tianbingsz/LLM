# -*- coding: utf-8 -*-
"""sft_train_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15fm-SKbHHITxe_-9LUe-H9SSsLsqGtsb

# Supervised Fine-Tuning with GPT2 writing from Scratch
"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch.nn.functional as F

"""## LOAD TOKENIZER and MODEL"""

model_name = "GPT2"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'left'
model.to(device)

"""## Alphaca dataset

"""

!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json

from datasets import load_dataset
import json
import numpy as np


class AlpacaDataset(Dataset):
    def __init__(self, tokenizer, num_samples=32, max_len=128):
        super().__init__()
        self.tokenizer = tokenizer
        self.num_samples = num_samples
        self.max_len = max_len

        formatted_data = self._load_dataset()
        self._tokenize_dataset(formatted_data)

    def _load_dataset(self):
        # dataset = load_dataset("tatsu-lab/alpaca", split='train')
        with open('alpaca_data.json', 'r') as f:
            dataset = json.load(f)
        # pick num_samples random sample
        indices = np.random.randint(0, len(dataset), (self.num_samples, ))

        # Format as prompt + response
        formatted_data = []
        for idx in indices:
            example = dataset[int(idx)]
            instruction = example.get("instruction", "")
            input_text = example.get("input", "")
            output = example.get("output", "")

            if input_text:
                prompt = f"{instruction}\n\n{input_text}"
            else:
                prompt = instruction

            full_text = prompt.strip() + "\n\n" + output
            formatted_data.append({"text": full_text})

        return formatted_data

    def _tokenize_dataset(self, formatted_data):
        self.samples = []
        for example in formatted_data:
            encoded_text = self.tokenizer(
                example['text'],
                truncation=True,
                padding="max_length",
                max_length=self.max_len,
                return_tensors="pt",
            )

            input_ids = encoded_text.input_ids.squeeze(0)
            attention_mask = encoded_text.attention_mask.squeeze(0)
            labels = input_ids.clone() # For causal LM, labels are input_ids

            self.samples.append({
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "labels": labels,
            })

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

"""### Create dataset and dataloader

"""

num_samples = 10240
max_len = 128
batch_size = 32
dataset = AlpacaDataset(tokenizer, num_samples=num_samples, max_len=max_len)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

"""## Train SFT"""

def sft_forward(model, x, labels, attention_mask=None):
    """
        X: {"input_ids": input_ids, "attention_mask": attention_mask}
        input_ids: (B, T)
        attention_mask: (B, T)
        labels: (B, T)
    """
    outputs = model(**x)
    logits = outputs.logits
    batch_size, seq_len, vocab_size = logits.size()
    shift_logits = logits[:, :-1, :].contiguous() # (B, T-1, V)
    shift_labels = labels[:, 1:].contiguous() # (B, T-1, V), next token prediction

    # loss
    if attention_mask is None:
        loss = F.cross_entropy(shift_logits.view(-1, vocab_size),
                               shift_labels.view(-1)) # averaged scalar
        return loss

    # attention_mask is not None:
    loss = F.cross_entropy(
        shift_logits.view(-1, vocab_size),
        shift_labels.view(-1),
        reduce='none',
    ) # loss per token, (B * (T-1), )
    shift_mask = attention_mask[:, 1:].contiguous() # (B, T-1)
    loss = (loss * shift_mask.view(-1)).sum() / shift_mask.sum()
    return loss

def train_SFT(model, dataloader, lr=5e-5, epoches=3):
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    for epoch in range(epoches):
        for step, batch in enumerate(dataloader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            x = {'input_ids': input_ids, 'attention_mask': attention_mask}
            loss = sft_forward(model, x, labels, attention_mask=attention_mask)
            loss.backward()
            optimizer.step()

            print(f"[Epoch {epoch+1} Step {step+1}] Loss: {loss.item():.4f}")

train_SFT(model, dataloader, epoches=1)

save_dir = "SFT"

# Save model weights and config
model.save_pretrained(f"{save_dir}/model")

# Save tokenizer files
tokenizer.save_pretrained(f"{save_dir}/tokenizer")

"""## LOAD MODEL and INFERENCE"""

model = AutoModelForCausalLM.from_pretrained("SFT/model")
tokenizer = AutoTokenizer.from_pretrained("SFT/tokenizer")

# Set model to evaluation mode and move to device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device).eval()

def infer(prompt, max_new_tokens=100):
  # Tokenize input
  inputs = tokenizer(prompt, return_tensors='pt').to(device)
  # Generate response
  with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.8,
    )
  # Decode output
  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
  print("\n=== Model Output ===")
  print(generated_text)

prompt = "Explain the theory of relativity in simple terms."
infer(prompt)

prompt = "Teach me how to write a basic Python function."
infer(prompt)

prompt = "Write a poem about spring."
infer(prompt)