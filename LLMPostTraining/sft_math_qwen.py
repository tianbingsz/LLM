# -*- coding: utf-8 -*-
"""sft_math_qwen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sUeC-HXiso7D2oKByv5YYxhjI_J2Fai4

https://github.com/stanford-cs336/assignment5-alignment/blob/main/cs336_spring2025_assignment5_alignment.pdf

SFT on Math Dataset

2.1 Tokenizing Prompts
"""

R1_ZERO_PROMPT = """
A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.
The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.
User: {question}
Assistant: <think>
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def tokenize_prompt_and_output(prompt_strs, output_strs, tokenizer: AutoTokenizer):
    """
    Args:
        prompt_strs: list[str] List of prompt strings.
        output_strs: list[str] List of output strings.
        tokenizer: PreTrainedTokenizer Tokenizer to use for tokenization.
    Returns:
        dict[str, torch.Tensor]. Let prompt_and_output_lens be a list containing the lengths of
        the tokenized prompt and output strings. Then the returned dictionary should have the
        following keys:
        input_ids torch.Tensor of shape (batch_size, max(prompt_and_output_lens) - 1):
          the tokenized prompt and output strings, with the final token sliced off.
        labels torch.Tensor of shape (batch_size, max(prompt_and_output_lens) - 1):
          shifted input ids, i.e., the input ids without the first token.
        response_mask torch.Tensor of shape (batch_size, max(prompt_and_output_lens) -
          1): a mask on the response tokens in the labels.
    """

    input_ids_list = []
    response_masks = []

    for prompt, output in zip(prompt_strs, output_strs):
        prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)
        output_tokens = tokenizer.encode(output, add_special_tokens=False)

        input_ids = prompt_tokens + output_tokens
        input_ids_list.append(input_ids)

        response_mask = [0] * len(prompt_tokens) + [1] * len(output_tokens)
        response_masks.append(response_mask)

    max_len = max(len(ids) for ids in input_ids_list)

    padded_input_ids = []
    padded_labels = []
    padded_response_masks = []

    # padding
    for input_ids, response_mask in zip(input_ids_list, response_masks):
        padding_length = max_len - len(input_ids)

        # Slice lists properly:
        # input_ids without the last token
        input_ids_slice = input_ids[:-1] + [tokenizer.pad_token_id] * padding_length
        # labels are input_ids without the first token, pad with -100
        labels_slice = input_ids[1:] + [-100] * padding_length
        # response_mask padded with zeros, align with labels for loss calc
        response_mask_padded = response_mask[1:] + [0] * padding_length

        padded_input_ids.append(input_ids_slice)
        padded_labels.append(labels_slice)
        padded_response_masks.append(response_mask_padded)

    return {
        "input_ids": torch.tensor(padded_input_ids, dtype=torch.long),
        "labels": torch.tensor(padded_labels, dtype=torch.long),
        "response_masks": torch.tensor(padded_response_masks, dtype=torch.long)
    }

"""2.1.1 Math Dataset"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM
import json

class MathSFTDataset(Dataset):
    def __init__(self, jsonl_path, tokenizer):
      self.jsonl_path = jsonl_path
      self.tokenizer = tokenizer
      questions, answers = self._load_math_data()
      self.prompts = [R1_ZERO_PROMPT.format(question=question) for question in questions]
      self.responses = answers

      tokenized_data = tokenize_prompt_and_output(self.prompts, self.responses, tokenizer)
      self.input_ids = tokenized_data["input_ids"] # (B, T)
      self.labels = tokenized_data["labels"] # (B, T)
      self.responses_mask = tokenized_data["response_masks"] # (B, T)


    def __len__(self):
      return self.input_ids.shape[0]


    def __getitem__(self, idx):
      return self.input_ids[idx], self.labels[idx], self.responses_mask[idx]


    def _load_math_data(self):
      questions = []
      answers = []
      with open(self.jsonl_path, 'r') as f:
        for line in f.readlines():
          example = json.loads(line)
          questions.append(example['question'])
          answers.append(example['answer'])

      print(f"Loaded {len(questions)} MATH examples.")
      return questions, answers

"""2.2 Compute entropy

```
pi = softmax(logit_i)
H(p) = - \sum pi log(pi)
```


"""

import torch.nn.functional as F

def compute_entropy(logits: torch.Tensor) -> torch.Tensor:
    """
    Compute per-token entropy over the vocabulary for next-token predictions.

    Args:
        logits (torch.Tensor): Shape (batch_size, seq_len, vocab_size),
            unnormalized logits from the model.

    Returns:
        torch.Tensor: Shape (batch_size, seq_len),
            per-token entropy values.
    """
    # Compute logsumexp for numerical stability

    log_probs = F.log_softmax(logits, dim=-1) # (B, T, V)

    # H(p) = -sum(p * log(p))
    entropy = - torch.sum(torch.exp(log_probs) * log_probs, dim=-1) # (B, T)

    return entropy

"""2.3 Get Log prob for repsonse
```
log pθ(y | x) = log [softmax(fθ(x))]_y
```

"""

def get_response_log_probs(
    model,
    input_ids: torch.Tensor,
    labels: torch.Tensor,
    return_token_entropy: bool = False,
) -> dict[str, torch.Tensor]:
    with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
      logits = model(input_ids).logits  # (B, T, V)
      # print(f"logits.shape = {logits.shape}")

    log_probs = F.log_softmax(logits, dim=-1)  # (B, T, V)

    mask = labels != -100
    safe_labels = labels.clone()
    safe_labels[~mask] = 0  # placeholder, won't be used

    # Correctly assign the gathered log_probs
    selected_log_probs = torch.gather(log_probs, dim=-1, index=safe_labels.unsqueeze(-1)).squeeze(-1)
    selected_log_probs = selected_log_probs.masked_fill(~mask, 0.0)

    result = {
        "log_probs": selected_log_probs
    }

    if return_token_entropy:
        token_entropy = compute_entropy(logits)
        result["token_entropy"] = token_entropy

    return result

"""2.5 SFT microbatch train step. We are now ready to implement a single microbatch train step for SFT
(recall that for a train minibatch, we iterate over many microbatches if gradient_accumulation_steps >
1).
"""

def sft_microbatch_train_step(
    policy_log_probs: torch.Tensor,           # shape: (B, T)
    response_mask: torch.Tensor,              # shape: (B, T)
    gradient_accumulation_steps: int,
    normalize_constant: float = 1.0,
) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
    """
    Perform a single microbatch train step for SFT using cross-entropy loss.

    Args:
        policy_log_probs (torch.Tensor): Log-probs from model on gold tokens (B, T)
        response_mask (torch.Tensor): 1 for response tokens, 0 for prompt/padding (B, T)
        gradient_accumulation_steps (int): Number of microbatches per optimizer step.
        normalize_constant (float): Value to normalize the summed loss by.

    Returns:
        tuple[loss (scalar), metadata (dict)]
    """
    # Compute negative log-likelihood only for response tokens
    nll = - policy_log_probs * response_mask # (B, T)

    # Sum NLL over all response tokens, divide by normalize_constant
    total_loss = nll.sum() / normalize_constant

    # Scale for gradient accumulation
    scaled_loss = total_loss / gradient_accumulation_steps

    # Backward pass
    scaled_loss.backward()

    # Metadata for logging
    num_response_tokens = response_mask.sum().item()
    avg_loss_per_token = total_loss.item() /(num_response_tokens + 1e-8)
    metadata = {
        "loss": total_loss.item(),
        "avg_loss": avg_loss_per_token,
        "num_response_tokens": num_response_tokens,
    }

    return scaled_loss.item(), metadata

"""2.7 SFT Experiment"""

import json
import torch
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, get_scheduler

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Main Training Function ---
def run_sft(
    model_name: str = "Qwen/Qwen2.5-0.5B",
    train_path: str = "sample_data/train_1k.jsonl",
    output_dir: str = "checkpoints/sft-math-qwen",
    max_steps: int = 10,
    batch_size: int = 2,
    gradient_accumulation_steps: int = 4,
    learning_rate: float = 5e-5,
    eval_interval: int = 1,
    save_interval: int = 500,
):

    # Load model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    model = model.to(device)
    model.train()

    # Load datasets
    train_dataset = MathSFTDataset(train_path, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)


    optimizer = AdamW(model.parameters(), lr=learning_rate)
    lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=50, num_training_steps=max_steps)

    data_iter = iter(train_loader)
    step = 0
    while step < max_steps:
        optimizer.zero_grad()
        loss_accum = 0
        for micro_step in range(gradient_accumulation_steps):
          # Get next batch; restart iterator if needed
          try:
            input_ids, labels, response_masks = next(data_iter)
          except StopIteration:
            data_iter = iter(train_loader)
            input_ids, labels, response_masks = next(data_iter)

          input_ids = input_ids.to(device)
          labels = labels.to(device)
          response_masks = response_masks.to(device)

          selected_log_probs = get_response_log_probs(
              model,
              input_ids,
              labels
          )['log_probs']

          if hasattr(model, "require_backward_grad_sync"):
            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)


          # backward, masked loss
          loss, metadata = sft_microbatch_train_step(
              policy_log_probs=selected_log_probs,
              response_mask=response_masks,
              gradient_accumulation_steps=gradient_accumulation_steps
          )
          loss_accum += loss  # scalar only

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        lr_scheduler.step()

        step += 1

        if step % eval_interval == 0:
            print(f"Step {step}: loss = {loss_accum:.4f}")

        if step % save_interval == 0:
          model.save_pretrained(f"{output_dir}/checkpoint-{step}")
          tokenizer.save_pretrained(f"{output_dir}/checkpoint-{step}")

    model.save_pretrained(f"{output_dir}/final")
    tokenizer.save_pretrained(f"{output_dir}/final")

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"


run_sft(batch_size = 16, max_steps=50)