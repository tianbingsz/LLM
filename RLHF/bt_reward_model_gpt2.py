# -*- coding: utf-8 -*-
"""bt_reward_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vuki-JqcTYkievsLiQHNE5DTDB3okhOu

# Fine-Tune Bradley-Terry Reward model (Take Base Model GPT2 as an example)
"""

#!pip install -U datasets fsspec

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

import numpy as np
from transformers import GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification

"""## Prepare Dataset (Anthropic RLHF Dataset)

"""

from datasets import load_dataset

num_samples = 2048
raw_dataset = load_dataset("Anthropic/hh-rlhf", split="train[:2048]", trust_remote_code=True)

raw_dataset

raw_dataset[0]

class RewardDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=128):
      self.data = data
      self.tokenizer = tokenizer
      self.max_length = max_length

    def __len__(self):
      return len(self.data)

    def __getitem__(self, idx):
      sample = self.data[idx]
      # toknizer input_ids
      chosen = self.tokenizer(sample['chosen'],
                              truncation=True,
                              padding='max_length',
                              max_length=self.max_length,
                              return_tensors='pt')

      rejected = self.tokenizer(sample['rejected'],
                                truncation=True,
                                padding='max_length',
                                max_length=self.max_length,
                                return_tensors='pt')

      return {
          "chosen_ids": chosen['input_ids'].squeeze(0), # (max_length,)
          "chosen_mask": chosen['attention_mask'].squeeze(0),
          "rejected_ids": rejected['input_ids'].squeeze(0),
          "rejected_mask": rejected['attention_mask'].squeeze(0),
      }

"""## Load base model (GPT2ForSequenceClassification)


"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

config = GPT2Config.from_pretrained(model_name)
config.num_labels = 1
model = GPT2ForSequenceClassification.from_pretrained(model_name, config=config)
model.to(device)
model.config.pad_token_id = model.config.eos_token_id

"""## Training (Fine-Tuning)"""

def train_reward_model(model, dataloader, lr=1e-5, epoches=3):
  model.train()
  optimizer = optim.AdamW(model.parameters(), lr=lr)

  for epoch in range(epoches):
    for step, batch in enumerate(dataloader):
      optimizer.zero_grad() # reset grad

      chosen_ids = batch['chosen_ids'].to(device) # (B, T)
      chosen_mask = batch['chosen_mask'].to(device)
      rejected_ids = batch['rejected_ids'].to(device) # (B, T)
      rejected_mask = batch['rejected_mask'].to(device)

      # reward score for chosen and rejected seq, last score layer: out_feature=1
      chosen_scores = model(input_ids=chosen_ids, attention_mask=chosen_mask).logits # (B, 1)
      rejected_scores = model(input_ids=rejected_ids, attention_mask=rejected_mask).logits # (B, 1)

      # L = - log sigmoid(r(y) - r(y'))
      loss = -F.logsigmoid(chosen_scores - rejected_scores).mean() # (1,)
      loss.backward() # backprop
      optimizer.step() # update params

      print(f"Epoch: {epoch + 1}, Step: {step + 1}, Loss: {loss.item():.4f}")

max_length = 128
batch_size = 32
data = raw_dataset.shuffle(seed=42).select(range(num_samples))
dataset = RewardDataset(data, tokenizer, max_length=max_length)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

train_reward_model(model, dataloader, lr=5e-4, epoches=1)